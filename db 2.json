{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/ocean/source/404.html","path":"404.html","modified":1,"renderable":1},{"_id":"themes/ocean/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/404.styl","path":"css/404.styl","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/hexo-inverted.svg","path":"images/hexo-inverted.svg","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/hexo.svg","path":"images/hexo.svg","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/busuanzi-2.3.pure.min.js","path":"js/busuanzi-2.3.pure.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/lazyload.min.js","path":"js/lazyload.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/ocean.js","path":"js/ocean.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/pace.min.js","path":"js/pace.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.eot","path":"css/feathericon/feathericon.eot","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.ttf","path":"css/feathericon/feathericon.ttf","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff","path":"css/feathericon/feathericon.woff","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff2","path":"css/feathericon/feathericon.woff2","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/overlay-hero.png","path":"images/ocean/overlay-hero.png","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.svg","path":"css/feathericon/feathericon.svg","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/forrestgump.png","path":"images/forrestgump.png","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.ogv","path":"images/ocean/ocean.ogv","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/dz_hhh.jpeg","path":"images/ocean/dz_hhh.jpeg","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.png","path":"images/ocean/ocean.png","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.webm","path":"images/ocean/ocean.webm","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.mp4","path":"images/ocean/ocean.mp4","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"cc75fbdb977a72e3c33a32b977ec965c1597d5c5","modified":1564025592527},{"_id":"themes/ocean/README.md","hash":"8e711610c3600376d55c60b28ab20ffa8157b060","modified":1564711687256},{"_id":"themes/ocean/_config.yml","hash":"778d602eecf27a112b4fff0c10e298e743ce20e7","modified":1565073644799},{"_id":"themes/ocean/package.json","hash":"b993176f8c35bc3ab9dbd8642ec6cd125fcb447e","modified":1564711687261},{"_id":"source/_posts/.DS_Store","hash":"d4a4a42252497c943da4150023ab9ad99174357b","modified":1564994868111},{"_id":"source/_posts/aic-bic.md","hash":"4a10bdcebe5e1696f50f1f6f8b1eb3cca6568a13","modified":1565074941773},{"_id":"source/_posts/feature_test.md","hash":"d749c6feda9404e385d770f21386a7dd55e09d84","modified":1564025517387},{"_id":"source/_posts/linear-model.md","hash":"5da30aaeba228debce8dc50dd9a6b19737912a3c","modified":1565082117778},{"_id":"source/_posts/machine_learning.md","hash":"820ef2c0b054c84e1b5fef325e8821eb43622b9a","modified":1564025517387},{"_id":"source/_posts/random-forest.md","hash":"98e86de098e7b55a7f41f5f492c714f8758c1d2b","modified":1565079056599},{"_id":"source/_posts/read_data.md","hash":"c3c92772e826387147af734a3ad740fc1206749f","modified":1564025517387},{"_id":"source/_posts/timeseries.md","hash":"6b4724b5f7d7f87c1b56b8109be32e045bc4fb13","modified":1564976964819},{"_id":"themes/ocean/.git/COMMIT_EDITMSG","hash":"f223f5f5f3aa2bf592883b4f3e203c9360c61574","modified":1565077185779},{"_id":"themes/ocean/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1564711687250},{"_id":"themes/ocean/.git/config","hash":"686588db57bb2d68a492446ce94aa90be25c28fa","modified":1564711687252},{"_id":"themes/ocean/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1564711566900},{"_id":"themes/ocean/.git/index","hash":"8ff1846e38c8432cf4e574d56f96651df05b908f","modified":1565077185778},{"_id":"themes/ocean/.git/packed-refs","hash":"d24bfad6d6e0ee981aec58816e07017ba2b00993","modified":1564711687249},{"_id":"themes/ocean/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1564711687256},{"_id":"themes/ocean/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1564711687257},{"_id":"themes/ocean/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1564711687256},{"_id":"themes/ocean/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1564711687257},{"_id":"themes/ocean/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1564711687257},{"_id":"themes/ocean/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1564711687257},{"_id":"themes/ocean/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1564711687257},{"_id":"themes/ocean/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1564711687257},{"_id":"themes/ocean/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1564711687257},{"_id":"themes/ocean/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1564711687257},{"_id":"themes/ocean/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1564711687257},{"_id":"themes/ocean/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1564711687257},{"_id":"themes/ocean/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1564711687260},{"_id":"themes/ocean/layout/categories.ejs","hash":"0034642381558bd76d973868d3828aea5bf3a8cd","modified":1564711687260},{"_id":"themes/ocean/layout/index.ejs","hash":"dead30ea8014348cef977dcb44eea0ae0f0601c5","modified":1564711687260},{"_id":"themes/ocean/layout/layout.ejs","hash":"9ce598d82d973518e255fe64019b8523a2d65796","modified":1564711687260},{"_id":"themes/ocean/layout/post.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1564711687261},{"_id":"themes/ocean/layout/page.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1564711687260},{"_id":"themes/ocean/layout/tags.ejs","hash":"77240e3266c91cc10e136abb03a7b076a1c45908","modified":1564711687261},{"_id":"themes/ocean/source/404.html","hash":"788929fab7b99dd74575399f41cddae6f63ce1f4","modified":1564711687263},{"_id":"themes/ocean/source/favicon.ico","hash":"0f20298a6a4d1ebd7a7ae7b87d7a3ae9afec0623","modified":1564711687269},{"_id":"themes/ocean/layout/_partial/<%- theme.ocean.path %>dz_hhh.png","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1565082555289},{"_id":"themes/ocean/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1564711566900},{"_id":"themes/ocean/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1564711566900},{"_id":"themes/ocean/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1564711566903},{"_id":"themes/ocean/.git/logs/HEAD","hash":"b3e43b700511ed845f55b38c794456308082bc60","modified":1565077185780},{"_id":"themes/ocean/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1564711566899},{"_id":"themes/ocean/layout/_partial/after-footer.ejs","hash":"8a33afc5b80613896f5a5912ff09be586af127be","modified":1564711687258},{"_id":"themes/ocean/layout/_partial/archive-post.ejs","hash":"9be7173badcca6582c1136204adb3aa432aada21","modified":1564711687258},{"_id":"themes/ocean/layout/_partial/archive.ejs","hash":"d7221ce7a6f5989ded47f7d9b0f40778f897deb6","modified":1564711687258},{"_id":"themes/ocean/layout/_partial/article.ejs","hash":"7d8bf7ed4cd9677a8edba59c47e16cdeef864aa5","modified":1565073474820},{"_id":"themes/ocean/layout/_partial/footer.ejs","hash":"834f333dba6612e168056082ddd404d708c7e367","modified":1564736147126},{"_id":"themes/ocean/layout/_partial/head.ejs","hash":"ac644f55f1cacbf68c6347ca95b9aa87953afd31","modified":1565073813672},{"_id":"themes/ocean/layout/_partial/ocean.ejs","hash":"6eb3726ff06002ff1733e75d6a43bed550385db3","modified":1565082987689},{"_id":"themes/ocean/layout/_partial/sidebar.ejs","hash":"5778e5f469ab9a3e17359080e2e1245a2082ec96","modified":1564711687260},{"_id":"themes/ocean/layout/_partial/totop.ejs","hash":"72b960315983ee95363fa9cabe82f52916ac9ae3","modified":1564711687260},{"_id":"themes/ocean/source/css/404.styl","hash":"57a7cd3fb84232a34687963ae36c0e469e5918a3","modified":1564711687263},{"_id":"themes/ocean/screenshots/hexo-theme-ocean.jpg","hash":"13b5045d2120cac2f68849757f5e0af08938b7c6","modified":1564711687263},{"_id":"themes/ocean/source/css/_extend.styl","hash":"deb6aca91c40516f5d638008a72f9def42e5d081","modified":1564711687263},{"_id":"themes/ocean/source/css/_feathericon.styl","hash":"8494f0e869411781264868f08eda62fd838e0cee","modified":1564711687263},{"_id":"themes/ocean/source/css/_mixins.styl","hash":"6959409df2dd0a1ca05be0c0e9b2a884efdfb82d","modified":1564711687263},{"_id":"themes/ocean/source/css/_normalize.styl","hash":"b3337320133b7a336db7033aa6bbe94b054c0b21","modified":1564711687263},{"_id":"themes/ocean/source/css/_variables.styl","hash":"bc03c1fdc8c1d8568c506a7ca3aa149dbde02e04","modified":1564711687266},{"_id":"themes/ocean/source/css/style.styl","hash":"8dbe0f34f7e9a20b7b6fc5cc1458ad7931510ff5","modified":1564711687268},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1564711687268},{"_id":"themes/ocean/source/images/hexo-inverted.svg","hash":"525309ea3c7360f83d1d9df6d04c256d7171950d","modified":1564711687270},{"_id":"themes/ocean/source/images/hexo.svg","hash":"71e7204d04ccfe260f06ea5873484791cd5f404a","modified":1564711687270},{"_id":"themes/ocean/source/js/busuanzi-2.3.pure.min.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1564711687301},{"_id":"themes/ocean/source/js/lazyload.min.js","hash":"b801b3946fb9b72e03512c0663458e140e1fa77b","modified":1564711687302},{"_id":"themes/ocean/source/js/ocean.js","hash":"3457be62843930ad58997cd6fd387783285242c7","modified":1564711687302},{"_id":"themes/ocean/source/js/pace.min.js","hash":"d32ab818e0f97d3b0c80f5631fc23d8a0cb52795","modified":1564711687302},{"_id":"themes/ocean/source/js/search.js","hash":"118be0e0918532ac1225f62e1a0a6f0673e0b173","modified":1564711687302},{"_id":"source/_posts/linear-model/orth.png","hash":"c4eef4c96331fa979c9d773ac2a84557a3204d43","modified":1564995664206},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.js","hash":"b2b093d8f5ffeee250c8d0d3a2285a213318e4ea","modified":1564711687269},{"_id":"themes/ocean/source/js/jquery-2.0.3.min.js","hash":"800edb7787c30f4982bf38f2cb8f4f6fb61340e9","modified":1564711687301},{"_id":"themes/ocean/.git/objects/23/b90c59e258d0c33d67412a58b9bacaf019795a","hash":"d438f0c29bb67859d6ec0ccafff8c41d7c043da7","modified":1565077160794},{"_id":"themes/ocean/.git/objects/28/a966d41eef6fe849db5df61eb212bdc5a9419d","hash":"2828f3a580abaa4d99462c63c8bd298d8dc045f3","modified":1565077185779},{"_id":"themes/ocean/.git/objects/2c/c06299dec23c7d4cd2ca478c538671a957a198","hash":"02f51387a5df77800b00c3005dd9a3a8d95df8f2","modified":1565077160792},{"_id":"themes/ocean/.git/objects/5a/f8fcec5a9fa5028ab7585feefae6ced4c24ce6","hash":"af14ed8f80caba748d5a287930a07b659ce2eecd","modified":1565077185776},{"_id":"themes/ocean/.git/objects/60/f27e0cab07076f9f11c69becfed8cf1e84abe4","hash":"f67edf9a86a3ec19f79f406d0a7aca0ef798268e","modified":1565077160794},{"_id":"themes/ocean/.git/objects/99/009d068ee135465f6883e3675d467ac918d0e6","hash":"b114f765b7457c8516f88aac3303f782ce64899b","modified":1565077185778},{"_id":"themes/ocean/.git/objects/b5/a8368f3a2ca6b6d9c85e2cf05f5b595ea70278","hash":"b1a06e8a3519d51f5bd2ca4536e6e048893e50d1","modified":1565077185777},{"_id":"themes/ocean/.git/objects/f4/79507539496dabe1dfec2db08fd02ac2736dd4","hash":"ce76629ecafaa79c2b90f3f42036230249a55867","modified":1565077185777},{"_id":"themes/ocean/.git/objects/f6/e543fca3dd12ed8c47bc201a32774ea95c90f4","hash":"64d8f57984f2440cb16217876d42a90815c719b4","modified":1565077160793},{"_id":"themes/ocean/.git/objects/fa/20d5673033e2431d4fe945fc27f259deb191c7","hash":"1957dd04412716092a3187f6974a5709bb8c55a8","modified":1565077160793},{"_id":"themes/ocean/.git/objects/pack/pack-9cc67a3a009ce4a7eab3bf2787338d141b31feb7.idx","hash":"6ff7947e4bb9987fca29b8a6b030d6523c5aa356","modified":1564711687236},{"_id":"themes/ocean/.git/refs/heads/master","hash":"ec5387aa03acf5bd4623e046c6616417b3c28ef8","modified":1565077185780},{"_id":"themes/ocean/layout/_partial/post/albums.ejs","hash":"26812d13f8b7edda90c0603aa1bb7a592ee2deed","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/gallery.ejs","hash":"5f8487fe7bed9a09001c6655244ff35f583cf1eb","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/busuanzi.ejs","hash":"44f361700640a5113fa17e6a346ade8208564aa4","modified":1564740460359},{"_id":"themes/ocean/layout/_partial/post/category.ejs","hash":"85f0ebeceee1c32623bfa1e4170dbe1e34442fea","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/gitalk.ejs","hash":"e36d149ad83c3a52562dbef61a0083957eb24578","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/nav.ejs","hash":"e59198918e92ef92156aeefbf6023584ac1cae64","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/search.ejs","hash":"2c9d19d1685e834aa2020998da2a2d259ce9b9ff","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1564711687260},{"_id":"themes/ocean/layout/_partial/post/title.ejs","hash":"53ccbfc6f1c424fb4dd609c1a61ffb69841403cc","modified":1564711687260},{"_id":"themes/ocean/layout/_partial/post/topping.ejs","hash":"bacd7e1d09397cfb32d97b5f3296f3ac538e57ea","modified":1564711687260},{"_id":"themes/ocean/source/css/_partial/albums.styl","hash":"0659d5f7469f24a415354ff767d949926465d515","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/apple.styl","hash":"e06dce604cc58ec39d677e4e59910c2725684901","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/archive.styl","hash":"8aefdcf2d542ad839018c2c58511e3318a38490d","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/article.styl","hash":"4c4ab4166987abe8a4e3df5bec11ec072f35a3ea","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/articles.styl","hash":"7bf289013d304505984b251be725b49165a694fd","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/categories.styl","hash":"f0c898823a5ddc37ae6bf76cc34ce8e50dd30885","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/float.styl","hash":"d888df89a172e4c8119cb8740fc1eae1a9539157","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/footer.styl","hash":"24779cbce1012d4f35ffc6b3ec0830cbc2ea3b3f","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/gallery.styl","hash":"7bdc2c9fb4971dbd7511c5cbb69bd611f20db591","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/gitalk.styl","hash":"3706eef2e0541493f1679a30241d279e29dfdc17","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/highlight.styl","hash":"c6e99fd23056fb01177aeefbc5dd4a8e88cf8f81","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/layou.styl","hash":"445cccbdf3a01c340688f28124ac156b0b6d0214","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/lists.styl","hash":"087f08e0ce9aca48e096dabca6eed2368b5bcd6b","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/mobile.styl","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/navbar.styl","hash":"4b41a8cde18b10d6e5f1a2adab0f4d3e81869b7e","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/ocean.styl","hash":"69ba351909c73eb1e04510facc9b35dd584198e0","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/pace.styl","hash":"e326918ba276ee332d0598d8193ccd8353e7d916","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/search.styl","hash":"011aaf21942dfff514ed4e98ce20142efbdd1b71","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/sidebar.styl","hash":"600c70f1de82da5223af290d47a583f9c379d188","modified":1564711687266},{"_id":"themes/ocean/source/css/_partial/tag.styl","hash":"925af8beede44ab53fe3cd0a5c472d2baa03baec","modified":1564711687266},{"_id":"themes/ocean/source/css/_partial/totop.styl","hash":"4bae031b6852384666cdf36e98c6bbbba1281453","modified":1564711687266},{"_id":"themes/ocean/source/css/feathericon/feathericon.eot","hash":"e2a01ae6f849841bc7a9fd21e5b7b450f1ded19b","modified":1564711687266},{"_id":"themes/ocean/source/css/feathericon/feathericon.ttf","hash":"d0d80c3c960d7d45e6bd7fa428d8a6a8c8245b2d","modified":1564711687267},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff","hash":"d22fe861e47afd92969ab46c7cbb7ea9c225aaf8","modified":1564711687268},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff2","hash":"2c11c45331d914ee38ad42ccf966132a508b5596","modified":1564711687268},{"_id":"themes/ocean/source/images/ocean/overlay-hero.png","hash":"92481a1848c35be96a693af11f77265323a7c189","modified":1564711687301},{"_id":"themes/ocean/source/css/feathericon/feathericon.svg","hash":"c113006c6822451802c8457128c352c0e4934453","modified":1564711687267},{"_id":"themes/ocean/.git/logs/refs/heads/master","hash":"b3e43b700511ed845f55b38c794456308082bc60","modified":1565077185780},{"_id":"themes/ocean/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1564711687250},{"_id":"themes/ocean/source/images/forrestgump.png","hash":"18ad6a8ba815878e36a0d5562136dc4fb8920c12","modified":1564711687270},{"_id":"themes/ocean/.git/logs/refs/remotes/origin/HEAD","hash":"add0d37464a85f3b7bc22cd37a8df8a31c03eb25","modified":1564711687250},{"_id":"themes/ocean/source/images/ocean/ocean.ogv","hash":"9c6b5d6b0544472cee39f5eafac2d5cbba5fd86b","modified":1564711687287},{"_id":"themes/ocean/source/images/ocean/dz_hhh.jpeg","hash":"dd9dcdd26b4c6a0dcce182a5a730e184a9ccef61","modified":1565078323580},{"_id":"themes/ocean/source/images/ocean/ocean.png","hash":"8245d07f812625d19b48ad2d00f8191f2aa4d304","modified":1564711687291},{"_id":"themes/ocean/source/images/ocean/ocean.webm","hash":"65aa2b6483e0151611899e31571057334c60d9e4","modified":1564711687300},{"_id":"themes/ocean/source/images/ocean/ocean.mp4","hash":"1e89cac2d652005d9dafd3ecb4dd460a8ff6d6af","modified":1564711687283},{"_id":"themes/ocean/.git/objects/pack/pack-9cc67a3a009ce4a7eab3bf2787338d141b31feb7.pack","hash":"95907dbd7f581b62a8b5eda05c0975e2b50948a0","modified":1564711687235}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"AIC and BIC","date":"2019-07-29T03:51:17.000Z","_content":"\n## AIC \n\n\n## Note \nAs you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form \n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\"/>\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\"/>\n</p>\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\"/>\n</p>\n\n\nwhere $f(x | \\theta)$ is joint probability density function, $k$ is the number of estimated parameter in our model.\n\nFor AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.\n\nAIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.\n\nAIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.\n\nSo what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.\n\n","source":"_posts/aic-bic.md","raw":"---\ntitle: AIC and BIC\ndate: 2019-07-29 11:51:17\ntags:\n---\n\n## AIC \n\n\n## Note \nAs you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form \n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\"/>\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\"/>\n</p>\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\"/>\n</p>\n\n\nwhere $f(x | \\theta)$ is joint probability density function, $k$ is the number of estimated parameter in our model.\n\nFor AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.\n\nAIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.\n\nAIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.\n\nSo what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.\n\n","slug":"aic-bic","published":1,"updated":"2019-08-06T07:02:21.773Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbf0000c3dl797lg58c","content":"<h2 id=\"aic\"><a class=\"markdownIt-Anchor\" href=\"#aic\"></a> AIC</h2>\n<h2 id=\"note\"><a class=\"markdownIt-Anchor\" href=\"#note\"></a> Note</h2>\n<p>As you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form</p>\n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\">\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x | \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is joint probability density function, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> is the number of estimated parameter in our model.</p>\n<p>For AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.</p>\n<p>AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.</p>\n<p>AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.</p>\n<p>So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"aic\"><a class=\"markdownIt-Anchor\" href=\"#aic\"></a> AIC</h2>\n<h2 id=\"note\"><a class=\"markdownIt-Anchor\" href=\"#note\"></a> Note</h2>\n<p>As you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form</p>\n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\">\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x | \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is joint probability density function, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> is the number of estimated parameter in our model.</p>\n<p>For AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.</p>\n<p>AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.</p>\n<p>AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.</p>\n<p>So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.</p>\n"},{"title":"Find important feature","_content":"## Select feature\n### Reason to do feature selection\n1. Because a large number of features may cost long training time\n2. Increasing number of features may increase the risk of overfitting\nIt can also help us to reduce the dimension of our dataset without loss main information.\n### Methods of feature selection\n#### P-value\nP-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.\n```python\nimport statsmodels.api as sm\n#Adding constant column of ones, mandatory for sm.OLS model\nx = pd.DataFrame() # feature df\ny = pd.DataFrame() # target df\nx_1 = sm.add_constant(x) # add a constant column to df x\n#Fitting sm.OLS model\nmodel = sm.OLS(np.array(y), np.array(x_1)).fit()\n# Which prints out the p-value of each features in this model as an array \nmodel.pvalues\n```\n\nWrite a function to select feature based on p-value:\n```python\n# x is feature df\n# y is target df\n# sl is significant level\ndef backwardElimination(x, y, sl):\n    cols = list(x.columns)\n    pmax = 1\n    while (len(cols) > 0):\n        p= []                                                                                   \n        X_1 = x[cols]\n        X_1 = sm.add_constant(X_1)\n        model = sm.OLS(y, X_1).fit()\n        p = pd.Series(model.pvalues.values[1:], index = cols)      \n        pmax = max(p)\n        # .idxmax returns the index of maximum\n        feature_with_p_max = p.idxmax()\n        if(pmax > sl):\n            cols.remove(feature_with_p_max)\n        else:\n            break\n    selected_features_BE = cols\n    print(selected_features_BE)\n\nbackwardElimination(x, y, 0.05)\n```\n\n\n#### F-test\nF-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.\n\nHere introduced the [skitlearn package](https://scikit-learn.org/stable/), we will use [F-test](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) to find first K best features:\n\nFor continues data type\n```python\nfrom sklearn import sklearn.feature_selection\nsklearn.feature_selection.f_regression(x, y) # where x is feature df(n_sample * n_features), y is target df (n_samples)\n# output is set of F-score and p-value for each F-score\n```\nFor classification data\n```python\nsklearn.feature_selection.f_classif(x, y) # same with f_regression\nsklearn.feature_selection.chi2(x, y) # if x is sparse, then only use chi2 can still keep it sparsity. \n```\nF-score is good for linear relation\n\n#### Mutual infomation\nIf x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.\nMore detail in [sklearn Mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\nWhich is good for non-linear relation\n```python\n# x is feature df, y is target df\n# For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X\n# n_neighbor higher values reduce variance of the estimation\nsklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=3, copy=True, random_state=None)\nsklearn.feature_selection.mututal_info_classif(x, y)\n\n# output is estimated MI between each feature and target\n```\n#### Variance threshold\nWhich is only care about feature itself: if it not vary a lot, then it has poor predictive power.\n```python\nsklearn.feature_selection.VarianceThreshold\n```\n\n","source":"_posts/feature_test.md","raw":"---\ntitle: Find important feature\n---\n## Select feature\n### Reason to do feature selection\n1. Because a large number of features may cost long training time\n2. Increasing number of features may increase the risk of overfitting\nIt can also help us to reduce the dimension of our dataset without loss main information.\n### Methods of feature selection\n#### P-value\nP-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.\n```python\nimport statsmodels.api as sm\n#Adding constant column of ones, mandatory for sm.OLS model\nx = pd.DataFrame() # feature df\ny = pd.DataFrame() # target df\nx_1 = sm.add_constant(x) # add a constant column to df x\n#Fitting sm.OLS model\nmodel = sm.OLS(np.array(y), np.array(x_1)).fit()\n# Which prints out the p-value of each features in this model as an array \nmodel.pvalues\n```\n\nWrite a function to select feature based on p-value:\n```python\n# x is feature df\n# y is target df\n# sl is significant level\ndef backwardElimination(x, y, sl):\n    cols = list(x.columns)\n    pmax = 1\n    while (len(cols) > 0):\n        p= []                                                                                   \n        X_1 = x[cols]\n        X_1 = sm.add_constant(X_1)\n        model = sm.OLS(y, X_1).fit()\n        p = pd.Series(model.pvalues.values[1:], index = cols)      \n        pmax = max(p)\n        # .idxmax returns the index of maximum\n        feature_with_p_max = p.idxmax()\n        if(pmax > sl):\n            cols.remove(feature_with_p_max)\n        else:\n            break\n    selected_features_BE = cols\n    print(selected_features_BE)\n\nbackwardElimination(x, y, 0.05)\n```\n\n\n#### F-test\nF-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.\n\nHere introduced the [skitlearn package](https://scikit-learn.org/stable/), we will use [F-test](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) to find first K best features:\n\nFor continues data type\n```python\nfrom sklearn import sklearn.feature_selection\nsklearn.feature_selection.f_regression(x, y) # where x is feature df(n_sample * n_features), y is target df (n_samples)\n# output is set of F-score and p-value for each F-score\n```\nFor classification data\n```python\nsklearn.feature_selection.f_classif(x, y) # same with f_regression\nsklearn.feature_selection.chi2(x, y) # if x is sparse, then only use chi2 can still keep it sparsity. \n```\nF-score is good for linear relation\n\n#### Mutual infomation\nIf x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.\nMore detail in [sklearn Mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\nWhich is good for non-linear relation\n```python\n# x is feature df, y is target df\n# For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X\n# n_neighbor higher values reduce variance of the estimation\nsklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=3, copy=True, random_state=None)\nsklearn.feature_selection.mututal_info_classif(x, y)\n\n# output is estimated MI between each feature and target\n```\n#### Variance threshold\nWhich is only care about feature itself: if it not vary a lot, then it has poor predictive power.\n```python\nsklearn.feature_selection.VarianceThreshold\n```\n\n","slug":"feature_test","published":1,"date":"2019-07-25T03:31:57.387Z","updated":"2019-07-25T03:31:57.387Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbn0001c3dlbcq7mnnw","content":"<h2 id=\"select-feature\"><a class=\"markdownIt-Anchor\" href=\"#select-feature\"></a> Select feature</h2>\n<h3 id=\"reason-to-do-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#reason-to-do-feature-selection\"></a> Reason to do feature selection</h3>\n<ol>\n<li>Because a large number of features may cost long training time</li>\n<li>Increasing number of features may increase the risk of overfitting<br>\nIt can also help us to reduce the dimension of our dataset without loss main information.</li>\n</ol>\n<h3 id=\"methods-of-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#methods-of-feature-selection\"></a> Methods of feature selection</h3>\n<h4 id=\"p-value\"><a class=\"markdownIt-Anchor\" href=\"#p-value\"></a> P-value</h4>\n<p>P-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> statsmodels.api <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"><span class=\"comment\">#Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class=\"line\">x = pd.DataFrame() <span class=\"comment\"># feature df</span></span><br><span class=\"line\">y = pd.DataFrame() <span class=\"comment\"># target df</span></span><br><span class=\"line\">x_1 = sm.add_constant(x) <span class=\"comment\"># add a constant column to df x</span></span><br><span class=\"line\"><span class=\"comment\">#Fitting sm.OLS model</span></span><br><span class=\"line\">model = sm.OLS(np.array(y), np.array(x_1)).fit()</span><br><span class=\"line\"><span class=\"comment\"># Which prints out the p-value of each features in this model as an array </span></span><br><span class=\"line\">model.pvalues</span><br></pre></td></tr></table></figure>\n<p>Write a function to select feature based on p-value:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df</span></span><br><span class=\"line\"><span class=\"comment\"># y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># sl is significant level</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backwardElimination</span><span class=\"params\">(x, y, sl)</span>:</span></span><br><span class=\"line\">    cols = list(x.columns)</span><br><span class=\"line\">    pmax = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (len(cols) &gt; <span class=\"number\">0</span>):</span><br><span class=\"line\">        p= []                                                                                   </span><br><span class=\"line\">        X_1 = x[cols]</span><br><span class=\"line\">        X_1 = sm.add_constant(X_1)</span><br><span class=\"line\">        model = sm.OLS(y, X_1).fit()</span><br><span class=\"line\">        p = pd.Series(model.pvalues.values[<span class=\"number\">1</span>:], index = cols)      </span><br><span class=\"line\">        pmax = max(p)</span><br><span class=\"line\">        <span class=\"comment\"># .idxmax returns the index of maximum</span></span><br><span class=\"line\">        feature_with_p_max = p.idxmax()</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pmax &gt; sl):</span><br><span class=\"line\">            cols.remove(feature_with_p_max)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    selected_features_BE = cols</span><br><span class=\"line\">    print(selected_features_BE)</span><br><span class=\"line\"></span><br><span class=\"line\">backwardElimination(x, y, <span class=\"number\">0.05</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"f-test\"><a class=\"markdownIt-Anchor\" href=\"#f-test\"></a> F-test</h4>\n<p>F-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.</p>\n<p>Here introduced the <a href=\"https://scikit-learn.org/stable/\" target=\"_blank\" rel=\"noopener\">skitlearn package</a>, we will use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html\" target=\"_blank\" rel=\"noopener\">F-test</a> to find first K best features:</p>\n<p>For continues data type</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> sklearn.feature_selection</span><br><span class=\"line\">sklearn.feature_selection.f_regression(x, y) <span class=\"comment\"># where x is feature df(n_sample * n_features), y is target df (n_samples)</span></span><br><span class=\"line\"><span class=\"comment\"># output is set of F-score and p-value for each F-score</span></span><br></pre></td></tr></table></figure>\n<p>For classification data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.f_classif(x, y) <span class=\"comment\"># same with f_regression</span></span><br><span class=\"line\">sklearn.feature_selection.chi2(x, y) <span class=\"comment\"># if x is sparse, then only use chi2 can still keep it sparsity.</span></span><br></pre></td></tr></table></figure>\n<p>F-score is good for linear relation</p>\n<h4 id=\"mutual-infomation\"><a class=\"markdownIt-Anchor\" href=\"#mutual-infomation\"></a> Mutual infomation</h4>\n<p>If x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.<br>\nMore detail in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression\" target=\"_blank\" rel=\"noopener\">sklearn Mutual information</a><br>\nWhich is good for non-linear relation</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df, y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X</span></span><br><span class=\"line\"><span class=\"comment\"># n_neighbor higher values reduce variance of the estimation</span></span><br><span class=\"line\">sklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=<span class=\"number\">3</span>, copy=<span class=\"literal\">True</span>, random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\">sklearn.feature_selection.mututal_info_classif(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output is estimated MI between each feature and target</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"variance-threshold\"><a class=\"markdownIt-Anchor\" href=\"#variance-threshold\"></a> Variance threshold</h4>\n<p>Which is only care about feature itself: if it not vary a lot, then it has poor predictive power.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"select-feature\"><a class=\"markdownIt-Anchor\" href=\"#select-feature\"></a> Select feature</h2>\n<h3 id=\"reason-to-do-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#reason-to-do-feature-selection\"></a> Reason to do feature selection</h3>\n<ol>\n<li>Because a large number of features may cost long training time</li>\n<li>Increasing number of features may increase the risk of overfitting<br>\nIt can also help us to reduce the dimension of our dataset without loss main information.</li>\n</ol>\n<h3 id=\"methods-of-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#methods-of-feature-selection\"></a> Methods of feature selection</h3>\n<h4 id=\"p-value\"><a class=\"markdownIt-Anchor\" href=\"#p-value\"></a> P-value</h4>\n<p>P-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> statsmodels.api <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"><span class=\"comment\">#Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class=\"line\">x = pd.DataFrame() <span class=\"comment\"># feature df</span></span><br><span class=\"line\">y = pd.DataFrame() <span class=\"comment\"># target df</span></span><br><span class=\"line\">x_1 = sm.add_constant(x) <span class=\"comment\"># add a constant column to df x</span></span><br><span class=\"line\"><span class=\"comment\">#Fitting sm.OLS model</span></span><br><span class=\"line\">model = sm.OLS(np.array(y), np.array(x_1)).fit()</span><br><span class=\"line\"><span class=\"comment\"># Which prints out the p-value of each features in this model as an array </span></span><br><span class=\"line\">model.pvalues</span><br></pre></td></tr></table></figure>\n<p>Write a function to select feature based on p-value:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df</span></span><br><span class=\"line\"><span class=\"comment\"># y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># sl is significant level</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backwardElimination</span><span class=\"params\">(x, y, sl)</span>:</span></span><br><span class=\"line\">    cols = list(x.columns)</span><br><span class=\"line\">    pmax = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (len(cols) &gt; <span class=\"number\">0</span>):</span><br><span class=\"line\">        p= []                                                                                   </span><br><span class=\"line\">        X_1 = x[cols]</span><br><span class=\"line\">        X_1 = sm.add_constant(X_1)</span><br><span class=\"line\">        model = sm.OLS(y, X_1).fit()</span><br><span class=\"line\">        p = pd.Series(model.pvalues.values[<span class=\"number\">1</span>:], index = cols)      </span><br><span class=\"line\">        pmax = max(p)</span><br><span class=\"line\">        <span class=\"comment\"># .idxmax returns the index of maximum</span></span><br><span class=\"line\">        feature_with_p_max = p.idxmax()</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pmax &gt; sl):</span><br><span class=\"line\">            cols.remove(feature_with_p_max)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    selected_features_BE = cols</span><br><span class=\"line\">    print(selected_features_BE)</span><br><span class=\"line\"></span><br><span class=\"line\">backwardElimination(x, y, <span class=\"number\">0.05</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"f-test\"><a class=\"markdownIt-Anchor\" href=\"#f-test\"></a> F-test</h4>\n<p>F-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.</p>\n<p>Here introduced the <a href=\"https://scikit-learn.org/stable/\" target=\"_blank\" rel=\"noopener\">skitlearn package</a>, we will use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html\" target=\"_blank\" rel=\"noopener\">F-test</a> to find first K best features:</p>\n<p>For continues data type</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> sklearn.feature_selection</span><br><span class=\"line\">sklearn.feature_selection.f_regression(x, y) <span class=\"comment\"># where x is feature df(n_sample * n_features), y is target df (n_samples)</span></span><br><span class=\"line\"><span class=\"comment\"># output is set of F-score and p-value for each F-score</span></span><br></pre></td></tr></table></figure>\n<p>For classification data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.f_classif(x, y) <span class=\"comment\"># same with f_regression</span></span><br><span class=\"line\">sklearn.feature_selection.chi2(x, y) <span class=\"comment\"># if x is sparse, then only use chi2 can still keep it sparsity.</span></span><br></pre></td></tr></table></figure>\n<p>F-score is good for linear relation</p>\n<h4 id=\"mutual-infomation\"><a class=\"markdownIt-Anchor\" href=\"#mutual-infomation\"></a> Mutual infomation</h4>\n<p>If x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.<br>\nMore detail in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression\" target=\"_blank\" rel=\"noopener\">sklearn Mutual information</a><br>\nWhich is good for non-linear relation</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df, y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X</span></span><br><span class=\"line\"><span class=\"comment\"># n_neighbor higher values reduce variance of the estimation</span></span><br><span class=\"line\">sklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=<span class=\"number\">3</span>, copy=<span class=\"literal\">True</span>, random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\">sklearn.feature_selection.mututal_info_classif(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output is estimated MI between each feature and target</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"variance-threshold\"><a class=\"markdownIt-Anchor\" href=\"#variance-threshold\"></a> Variance threshold</h4>\n<p>Which is only care about feature itself: if it not vary a lot, then it has poor predictive power.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure>\n"},{"title":"Multiple Linear Model","date":"2019-08-05T07:04:51.000Z","_content":"\n## Multiple Linear Regression Model\nMultiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.\n\nWhere the formula for Multiple linear regression model is:\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\"/>\n</p> \nfor i = 1,...,n is the number of observations or data. p is the number of dependent variables. \n\nHence the matrix notation for multiple linear regression is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\"/>\n</p> \n\nWhich can also write in simple statement:\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\"/>\n</p> \n\nUsing leat square, we need to minimise this function \n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\"/>\n</p>\n\nTo find the 'best'  $\\beta$\n\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\"/>\n</p> \n\n![A test image](linear-model/orth.png)\n\nThen the residuals $y-\\hat{y}$ are [orthogonal](http://mathworld.wolfram.com/Orthogonal.html) to the columns of $X$:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\"/>\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\"/>\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\"/>\n</p> \n\nThen we can define \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^TXy\"/>\n</p> \n\nThis of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.\n\nFor fitted $\\hat{y}$, we can plug in the $\\hat{\\beta}$\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\"/>\n</p> \n\nThe matrix $H$ (Hat-matrix) is a $n*n$ matrix, it maps the observed values $y$ onto the fitted value $\\hat{y}$\n\nAnd residuals can be written as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\"/>\n</p> \n\nThen we can try an example:\n\n","source":"_posts/linear-model.md","raw":"---\ntitle: Multiple Linear Model\ndate: 2019-08-05 15:04:51\ntags:\n---\n\n## Multiple Linear Regression Model\nMultiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.\n\nWhere the formula for Multiple linear regression model is:\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\"/>\n</p> \nfor i = 1,...,n is the number of observations or data. p is the number of dependent variables. \n\nHence the matrix notation for multiple linear regression is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\"/>\n</p> \n\nWhich can also write in simple statement:\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\"/>\n</p> \n\nUsing leat square, we need to minimise this function \n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\"/>\n</p>\n\nTo find the 'best'  $\\beta$\n\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\"/>\n</p> \n\n![A test image](linear-model/orth.png)\n\nThen the residuals $y-\\hat{y}$ are [orthogonal](http://mathworld.wolfram.com/Orthogonal.html) to the columns of $X$:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\"/>\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\"/>\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\"/>\n</p> \n\nThen we can define \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^TXy\"/>\n</p> \n\nThis of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.\n\nFor fitted $\\hat{y}$, we can plug in the $\\hat{\\beta}$\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\"/>\n</p> \n\nThe matrix $H$ (Hat-matrix) is a $n*n$ matrix, it maps the observed values $y$ onto the fitted value $\\hat{y}$\n\nAnd residuals can be written as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\"/>\n</p> \n\nThen we can try an example:\n\n","slug":"linear-model","published":1,"updated":"2019-08-06T09:01:57.778Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbp0002c3dl5h1hhwjy","content":"<h2 id=\"multiple-linear-regression-model\"><a class=\"markdownIt-Anchor\" href=\"#multiple-linear-regression-model\"></a> Multiple Linear Regression Model</h2>\n<p>Multiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.</p>\n<p>Where the formula for Multiple linear regression model is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\">\n</p> \nfor i = 1,...,n is the number of observations or data. p is the number of dependent variables. \n<p>Hence the matrix notation for multiple linear regression is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\">\n</p> \n<p>Which can also write in simple statement:</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\">\n</p> \n<p>Using leat square, we need to minimise this function</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\">\n</p>\n<p>To find the ‘best’  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span></p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\">\n</p> \n<p><img src=\"/2019/08/05/linear-model/orth.png\" alt=\"A test image\"></p>\n<p>Then the residuals <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> are <a href=\"http://mathworld.wolfram.com/Orthogonal.html\" target=\"_blank\" rel=\"noopener\">orthogonal</a> to the columns of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\">\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\">\n</p> \n<p>Then we can define</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^TXy\">\n</p> \n<p>This of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.</p>\n<p>For fitted <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>, we can plug in the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{\\beta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\">\n</p> \n<p>The matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> (Hat-matrix) is a <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi><mo>∗</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n*n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> matrix, it maps the observed values <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> onto the fitted value <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p>And residuals can be written as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\">\n</p> \n<p>Then we can try an example:</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"multiple-linear-regression-model\"><a class=\"markdownIt-Anchor\" href=\"#multiple-linear-regression-model\"></a> Multiple Linear Regression Model</h2>\n<p>Multiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.</p>\n<p>Where the formula for Multiple linear regression model is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\">\n</p> \nfor i = 1,...,n is the number of observations or data. p is the number of dependent variables. \n<p>Hence the matrix notation for multiple linear regression is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\">\n</p> \n<p>Which can also write in simple statement:</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\">\n</p> \n<p>Using leat square, we need to minimise this function</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\">\n</p>\n<p>To find the ‘best’  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span></p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\">\n</p> \n<p><img src=\"/2019/08/05/linear-model/orth.png\" alt=\"A test image\"></p>\n<p>Then the residuals <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> are <a href=\"http://mathworld.wolfram.com/Orthogonal.html\" target=\"_blank\" rel=\"noopener\">orthogonal</a> to the columns of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\">\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\">\n</p> \n<p>Then we can define</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^TXy\">\n</p> \n<p>This of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.</p>\n<p>For fitted <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>, we can plug in the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{\\beta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\">\n</p> \n<p>The matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> (Hat-matrix) is a <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi><mo>∗</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n*n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> matrix, it maps the observed values <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> onto the fitted value <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p>And residuals can be written as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\">\n</p> \n<p>Then we can try an example:</p>\n"},{"title":"Machine Learning","_content":"Before we use many ML algorithm, we sometimes need to preprocess the data\n\nImport all package\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels as sm\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n#import model package\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n```\n\n## Preprocessing\nFunction to encode the data type, change all object type features into dummy variables \n```python \ndef encoder(df):\n    for column in df.columns:\n        if df[column].dtype == type(object):\n            le = preprocessing.LabelEncoder()\n            df[column] = le.fit_transform(df[column])\n    return df\n```\nFunction to normalise the data\n```python\ndef normalised(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(df.values)\n    df_new = pd.DataFrame(x_scaled, columns= df.columns)\n    return df_new\n```\n\nOr we can use the package in scikit learn of [data transformation](https://scikit-learn.org/stable/data_transforms.html). Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.\n```python\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_tranform(x_train)\nsc.tranform(x_test)\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit_tranform(x_train)\nmms.tranform(x_test)\n```\n\nUsing PCA as an example of reducing dimension\n```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\npca.fit_transdorm()\n```\n\n\n\nSpliting the data frame into train and test to varify the goodness of model\n```python\n# a fixed randome_state num will have same train and test set \ntrain_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[\"prod_id\"]], test_size=0.2, random_state=42)\n```\nLogistic Regression Classification\n```python\ndef logistic_regression_classifier(train_x, train_y):\n    model = LogisticRegression(penalty='l2', solver = \"lbfgs\", multi_class='auto')\n    model.fit(train_x, train_y)\n    return model\n\nlgr_model = logistic_regression_classifier(train_x, train_y)\npred_y = lgr_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nNaive Bayes Classification\n```python\ndef naive_bayes_classifier(x, y):\n    model = GaussianNB()\n    model.fit(x, y)\n    return model\n\nnb_model = naive_bayes_classifier(train_x, train_y)\npred_y = nb_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nKNN K-Nearest Neighbors Classification\n```python\ndef knn_classifier(x, y):\n    model = KNeighborsClassifier()\n    model.fit(train_x, train_y)\n    return model \n\nknn_model = knn_classifier(train_x, train_y)\npred_y = knn_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nRandom Forest Classifier\n```python\ndef random_forest_classifier(train_x, train_y):\n    model = RandomForestClassifier(max_depth=2, random_state=0, )\n    model.fit(train_x, train_y)\n    return model\n\nrf_model = random_forest_classifier(train_x, train_y)\nrf_model.feature_importances_  # show feature importance for each feature\nmodel = SelectFromModel(rf_model, prefit=True) # select no zero coefficient features\nx_new = model.transform(x)\nx_new.shape()\n\ntrain_x_rf = train_x.iloc[:, my_list] # can mutually define my_list = [] to select important feature\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nDecision Tree Classifier\n```python\ndef decision_tree_classifier(train_x, train_y):\n    model = tree.DecisionTreeClassifier()\n    model.fit(train_x, train_y)\n    return model\n\ndt_model = decision_tree_classifier(train_x, train_y)\npred_y = dt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nGBDT-Gradient Boosting Decision Tree Classification\n```python\ndef gradient_boosting_classifier(train_x, train_y):\n    model = GradientBoostingClassifier(n_estimators=200)\n    model.fit(train_x, train_y)\n    return model\n\ngbdt_model = gradient_boosting_classifier(train_x, train_y)\npred_y = gbdt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nSVM-Support Vaector Machine Classification\n```python\ndef svm_classifier(train_x, train_y):\n    model = SVC(kernel='rbf', probability=True, gamma = \"auto\")\n    model.fit(train_x, train_y)\n    return model\n\nsvm_model = svm_classifier(train_x, train_y)\npred_y = svm_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```","source":"_posts/machine_learning.md","raw":"---\ntitle: Machine Learning \n---\nBefore we use many ML algorithm, we sometimes need to preprocess the data\n\nImport all package\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels as sm\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n#import model package\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n```\n\n## Preprocessing\nFunction to encode the data type, change all object type features into dummy variables \n```python \ndef encoder(df):\n    for column in df.columns:\n        if df[column].dtype == type(object):\n            le = preprocessing.LabelEncoder()\n            df[column] = le.fit_transform(df[column])\n    return df\n```\nFunction to normalise the data\n```python\ndef normalised(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(df.values)\n    df_new = pd.DataFrame(x_scaled, columns= df.columns)\n    return df_new\n```\n\nOr we can use the package in scikit learn of [data transformation](https://scikit-learn.org/stable/data_transforms.html). Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.\n```python\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_tranform(x_train)\nsc.tranform(x_test)\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit_tranform(x_train)\nmms.tranform(x_test)\n```\n\nUsing PCA as an example of reducing dimension\n```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\npca.fit_transdorm()\n```\n\n\n\nSpliting the data frame into train and test to varify the goodness of model\n```python\n# a fixed randome_state num will have same train and test set \ntrain_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[\"prod_id\"]], test_size=0.2, random_state=42)\n```\nLogistic Regression Classification\n```python\ndef logistic_regression_classifier(train_x, train_y):\n    model = LogisticRegression(penalty='l2', solver = \"lbfgs\", multi_class='auto')\n    model.fit(train_x, train_y)\n    return model\n\nlgr_model = logistic_regression_classifier(train_x, train_y)\npred_y = lgr_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nNaive Bayes Classification\n```python\ndef naive_bayes_classifier(x, y):\n    model = GaussianNB()\n    model.fit(x, y)\n    return model\n\nnb_model = naive_bayes_classifier(train_x, train_y)\npred_y = nb_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nKNN K-Nearest Neighbors Classification\n```python\ndef knn_classifier(x, y):\n    model = KNeighborsClassifier()\n    model.fit(train_x, train_y)\n    return model \n\nknn_model = knn_classifier(train_x, train_y)\npred_y = knn_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nRandom Forest Classifier\n```python\ndef random_forest_classifier(train_x, train_y):\n    model = RandomForestClassifier(max_depth=2, random_state=0, )\n    model.fit(train_x, train_y)\n    return model\n\nrf_model = random_forest_classifier(train_x, train_y)\nrf_model.feature_importances_  # show feature importance for each feature\nmodel = SelectFromModel(rf_model, prefit=True) # select no zero coefficient features\nx_new = model.transform(x)\nx_new.shape()\n\ntrain_x_rf = train_x.iloc[:, my_list] # can mutually define my_list = [] to select important feature\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nDecision Tree Classifier\n```python\ndef decision_tree_classifier(train_x, train_y):\n    model = tree.DecisionTreeClassifier()\n    model.fit(train_x, train_y)\n    return model\n\ndt_model = decision_tree_classifier(train_x, train_y)\npred_y = dt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nGBDT-Gradient Boosting Decision Tree Classification\n```python\ndef gradient_boosting_classifier(train_x, train_y):\n    model = GradientBoostingClassifier(n_estimators=200)\n    model.fit(train_x, train_y)\n    return model\n\ngbdt_model = gradient_boosting_classifier(train_x, train_y)\npred_y = gbdt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nSVM-Support Vaector Machine Classification\n```python\ndef svm_classifier(train_x, train_y):\n    model = SVC(kernel='rbf', probability=True, gamma = \"auto\")\n    model.fit(train_x, train_y)\n    return model\n\nsvm_model = svm_classifier(train_x, train_y)\npred_y = svm_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```","slug":"machine_learning","published":1,"date":"2019-07-25T03:31:57.387Z","updated":"2019-07-25T03:31:57.387Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbq0003c3dlhnrr5wwo","content":"<p>Before we use many ML algorithm, we sometimes need to preprocess the data</p>\n<p>Import all package</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> statsmodels <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split <span class=\"comment\"># Import train_test_split function</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> metrics <span class=\"comment\">#Import scikit-learn metrics module for accuracy calculation</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#import model package</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier <span class=\"comment\"># Import Decision Tree Classifier</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br></pre></td></tr></table></figure>\n<h2 id=\"preprocessing\"><a class=\"markdownIt-Anchor\" href=\"#preprocessing\"></a> Preprocessing</h2>\n<p>Function to encode the data type, change all object type features into dummy variables</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encoder</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> column <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> df[column].dtype == type(object):</span><br><span class=\"line\">            le = preprocessing.LabelEncoder()</span><br><span class=\"line\">            df[column] = le.fit_transform(df[column])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br></pre></td></tr></table></figure>\n<p>Function to normalise the data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalised</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class=\"line\">    x_scaled = min_max_scaler.fit_transform(df.values)</span><br><span class=\"line\">    df_new = pd.DataFrame(x_scaled, columns= df.columns)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_new</span><br></pre></td></tr></table></figure>\n<p>Or we can use the package in scikit learn of <a href=\"https://scikit-learn.org/stable/data_transforms.html\" target=\"_blank\" rel=\"noopener\">data transformation</a>. Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit_tranform(x_train)</span><br><span class=\"line\">sc.tranform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> MinMaxScaler</span><br><span class=\"line\">mms = MinMaxScaler()</span><br><span class=\"line\">mms.fit_tranform(x_train)</span><br><span class=\"line\">mms.tranform(x_test)</span><br></pre></td></tr></table></figure>\n<p>Using PCA as an example of reducing dimension</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\">pca = PCA(n_components = <span class=\"number\">3</span>)</span><br><span class=\"line\">pca.fit_transdorm()</span><br></pre></td></tr></table></figure>\n<p>Spliting the data frame into train and test to varify the goodness of model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># a fixed randome_state num will have same train and test set </span></span><br><span class=\"line\">train_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[<span class=\"string\">\"prod_id\"</span>]], test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br></pre></td></tr></table></figure>\n<p>Logistic Regression Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">logistic_regression_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = LogisticRegression(penalty=<span class=\"string\">'l2'</span>, solver = <span class=\"string\">\"lbfgs\"</span>, multi_class=<span class=\"string\">'auto'</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">lgr_model = logistic_regression_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = lgr_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Naive Bayes Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">naive_bayes_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = GaussianNB()</span><br><span class=\"line\">    model.fit(x, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">nb_model = naive_bayes_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = nb_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>KNN K-Nearest Neighbors Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">knn_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = KNeighborsClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model </span><br><span class=\"line\"></span><br><span class=\"line\">knn_model = knn_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = knn_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Random Forest Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_forest_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = RandomForestClassifier(max_depth=<span class=\"number\">2</span>, random_state=<span class=\"number\">0</span>, )</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">rf_model = random_forest_classifier(train_x, train_y)</span><br><span class=\"line\">rf_model.feature_importances_  <span class=\"comment\"># show feature importance for each feature</span></span><br><span class=\"line\">model = SelectFromModel(rf_model, prefit=<span class=\"literal\">True</span>) <span class=\"comment\"># select no zero coefficient features</span></span><br><span class=\"line\">x_new = model.transform(x)</span><br><span class=\"line\">x_new.shape()</span><br><span class=\"line\"></span><br><span class=\"line\">train_x_rf = train_x.iloc[:, my_list] <span class=\"comment\"># can mutually define my_list = [] to select important feature</span></span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Decision Tree Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decision_tree_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = tree.DecisionTreeClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">dt_model = decision_tree_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = dt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>GBDT-Gradient Boosting Decision Tree Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_boosting_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = GradientBoostingClassifier(n_estimators=<span class=\"number\">200</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">gbdt_model = gradient_boosting_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = gbdt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>SVM-Support Vaector Machine Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">svm_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = SVC(kernel=<span class=\"string\">'rbf'</span>, probability=<span class=\"literal\">True</span>, gamma = <span class=\"string\">\"auto\"</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">svm_model = svm_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = svm_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>Before we use many ML algorithm, we sometimes need to preprocess the data</p>\n<p>Import all package</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> statsmodels <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split <span class=\"comment\"># Import train_test_split function</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> metrics <span class=\"comment\">#Import scikit-learn metrics module for accuracy calculation</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#import model package</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier <span class=\"comment\"># Import Decision Tree Classifier</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br></pre></td></tr></table></figure>\n<h2 id=\"preprocessing\"><a class=\"markdownIt-Anchor\" href=\"#preprocessing\"></a> Preprocessing</h2>\n<p>Function to encode the data type, change all object type features into dummy variables</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encoder</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> column <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> df[column].dtype == type(object):</span><br><span class=\"line\">            le = preprocessing.LabelEncoder()</span><br><span class=\"line\">            df[column] = le.fit_transform(df[column])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br></pre></td></tr></table></figure>\n<p>Function to normalise the data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalised</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class=\"line\">    x_scaled = min_max_scaler.fit_transform(df.values)</span><br><span class=\"line\">    df_new = pd.DataFrame(x_scaled, columns= df.columns)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_new</span><br></pre></td></tr></table></figure>\n<p>Or we can use the package in scikit learn of <a href=\"https://scikit-learn.org/stable/data_transforms.html\" target=\"_blank\" rel=\"noopener\">data transformation</a>. Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit_tranform(x_train)</span><br><span class=\"line\">sc.tranform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> MinMaxScaler</span><br><span class=\"line\">mms = MinMaxScaler()</span><br><span class=\"line\">mms.fit_tranform(x_train)</span><br><span class=\"line\">mms.tranform(x_test)</span><br></pre></td></tr></table></figure>\n<p>Using PCA as an example of reducing dimension</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\">pca = PCA(n_components = <span class=\"number\">3</span>)</span><br><span class=\"line\">pca.fit_transdorm()</span><br></pre></td></tr></table></figure>\n<p>Spliting the data frame into train and test to varify the goodness of model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># a fixed randome_state num will have same train and test set </span></span><br><span class=\"line\">train_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[<span class=\"string\">\"prod_id\"</span>]], test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br></pre></td></tr></table></figure>\n<p>Logistic Regression Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">logistic_regression_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = LogisticRegression(penalty=<span class=\"string\">'l2'</span>, solver = <span class=\"string\">\"lbfgs\"</span>, multi_class=<span class=\"string\">'auto'</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">lgr_model = logistic_regression_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = lgr_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Naive Bayes Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">naive_bayes_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = GaussianNB()</span><br><span class=\"line\">    model.fit(x, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">nb_model = naive_bayes_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = nb_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>KNN K-Nearest Neighbors Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">knn_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = KNeighborsClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model </span><br><span class=\"line\"></span><br><span class=\"line\">knn_model = knn_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = knn_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Random Forest Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_forest_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = RandomForestClassifier(max_depth=<span class=\"number\">2</span>, random_state=<span class=\"number\">0</span>, )</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">rf_model = random_forest_classifier(train_x, train_y)</span><br><span class=\"line\">rf_model.feature_importances_  <span class=\"comment\"># show feature importance for each feature</span></span><br><span class=\"line\">model = SelectFromModel(rf_model, prefit=<span class=\"literal\">True</span>) <span class=\"comment\"># select no zero coefficient features</span></span><br><span class=\"line\">x_new = model.transform(x)</span><br><span class=\"line\">x_new.shape()</span><br><span class=\"line\"></span><br><span class=\"line\">train_x_rf = train_x.iloc[:, my_list] <span class=\"comment\"># can mutually define my_list = [] to select important feature</span></span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Decision Tree Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decision_tree_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = tree.DecisionTreeClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">dt_model = decision_tree_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = dt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>GBDT-Gradient Boosting Decision Tree Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_boosting_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = GradientBoostingClassifier(n_estimators=<span class=\"number\">200</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">gbdt_model = gradient_boosting_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = gbdt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>SVM-Support Vaector Machine Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">svm_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = SVC(kernel=<span class=\"string\">'rbf'</span>, probability=<span class=\"literal\">True</span>, gamma = <span class=\"string\">\"auto\"</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">svm_model = svm_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = svm_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>"},{"title":"random_forest","date":"2019-08-06T03:21:35.000Z","math":true,"_content":"123\n<!--more-->\n# Random Forest\n\nWhen $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\n$$\nx = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\n$$\n\n\n$$\n  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n$$","source":"_posts/random-forest.md","raw":"---\ntitle: random_forest\ndate: 2019-08-06 11:21:35\ntags:\nmath: true\n---\n123\n<!--more-->\n# Random Forest\n\nWhen $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\n$$\nx = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\n$$\n\n\n$$\n  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n$$","slug":"random-forest","published":1,"updated":"2019-08-06T08:10:56.599Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbr0004c3dlxd3r6y1n","content":"<p>123</p>\n<a id=\"more\"></a>\n<h1 id=\"random-forest\"><a class=\"markdownIt-Anchor\" href=\"#random-forest\"></a> Random Forest</h1>\n<p>When <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><mi mathvariant=\"normal\">≠</mi><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">a \\ne 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, there are two solutions to <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mi>b</mi><mi>x</mi><mo>+</mo><mi>c</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">ax^2 + bx + c = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.897438em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> and they are</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>=</mo><mfrac><mrow><mo>−</mo><mi>b</mi><mo>±</mo><msqrt><mrow><msup><mi>b</mi><mn>2</mn></msup><mo>−</mo><mn>4</mn><mi>a</mi><mi>c</mi></mrow></msqrt></mrow><mrow><mn>2</mn><mi>a</mi></mrow></mfrac><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.276389em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.590389em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord mathdefault\">b</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">±</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.913389em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathdefault\">b</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.740108em;\"><span style=\"top:-2.9890000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">4</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">c</span></span></span><span style=\"top:-2.873389em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width=\"400em\" height=\"1.08em\" viewbox=\"0 0 400000 1080\" preserveaspectratio=\"xMinYMin slice\"><path d=\"M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,\n-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,\n-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,\n35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,\n-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467\ns-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422\ns-65,47,-65,47z M834 80H400000v40H845z\"/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.12661100000000003em;\"><span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span><span class=\"mord\">.</span></span></span></span></span></p>\n<p class=\"katex-block katex-error\" title=\"ParseError: KaTeX parse error: Undefined control sequence: \\lnp at position 18: …max\\sum_{i=1}^l\\̲l̲n̲p̲(x_{i};\\theta)\n\">  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n</p>\n","site":{"data":{}},"excerpt":"<p>123</p>","more":"<h1 id=\"random-forest\"><a class=\"markdownIt-Anchor\" href=\"#random-forest\"></a> Random Forest</h1>\n<p>When <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><mi mathvariant=\"normal\">≠</mi><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">a \\ne 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, there are two solutions to <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mi>b</mi><mi>x</mi><mo>+</mo><mi>c</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">ax^2 + bx + c = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.897438em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> and they are</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>=</mo><mfrac><mrow><mo>−</mo><mi>b</mi><mo>±</mo><msqrt><mrow><msup><mi>b</mi><mn>2</mn></msup><mo>−</mo><mn>4</mn><mi>a</mi><mi>c</mi></mrow></msqrt></mrow><mrow><mn>2</mn><mi>a</mi></mrow></mfrac><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.276389em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.590389em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord mathdefault\">b</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">±</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.913389em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathdefault\">b</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.740108em;\"><span style=\"top:-2.9890000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">4</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">c</span></span></span><span style=\"top:-2.873389em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width=\"400em\" height=\"1.08em\" viewbox=\"0 0 400000 1080\" preserveaspectratio=\"xMinYMin slice\"><path d=\"M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,\n-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,\n-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,\n35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,\n-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467\ns-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422\ns-65,47,-65,47z M834 80H400000v40H845z\"/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.12661100000000003em;\"><span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span><span class=\"mord\">.</span></span></span></span></span></p>\n<p class=\"katex-block katex-error\" title=\"ParseError: KaTeX parse error: Undefined control sequence: \\lnp at position 18: …max\\sum_{i=1}^l\\̲l̲n̲p̲(x_{i};\\theta)\n\">  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n</p>"},{"title":"Read data","_content":"### csv file\nUsing Pandas package  [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\", encoding = \"utf-8\") # check encoding type such like \"utf-16\"\n ```\n\n### data from website\nReading data about house price and house feature from a website as an example:\n\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef soup_to_df(s):\n    #define column name\n    dfcols = ['outcode', 'last_published_date','latitude', 'longitude', 'post_town', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n    df_xml = pd.DataFrame(columns=dfcols)\n\n    for node in s.find_all(\"listing\"):\n        outcode =  node.find('outcode').get_text()\n        last_published_date = node.find('last_published_date').get_text()\n        latitude = node.find('latitude').get_text()\n        longitude = node.find('longitude').get_text()\n        post_town = node.find('post_town').get_text()\n        num_bathrooms = node.find('num_bathrooms').get_text()\n        num_bedrooms = node.find('num_bedrooms').get_text()\n        num_floors = node.find('num_floors').get_text()\n        num_recepts = node.find('num_recepts').get_text()\n        property_type = node.find('property_type').get_text()\n        street_name = node.find('street_name').get_text()\n        price = node.find('price').get_text()\n\n        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),\n                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=True)   \n    return df_xml\n\ndfcols = ['outcode', 'latitude', 'longitude', 'post_town', 'last_published_date', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n\ndf_all = pd.DataFrame(columns=dfcols)\n\nfor i in range(1, 58): #page range\n# Ctrl+Shift+I in that website to find the url for which we would like to scrape. \n    baseurl = f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&county=Somerset&country=England&listing_status=sale&include_sold=1&page_number={i}&page_size=100\"\n    page = requests.get(baseurl)\n    soup = BeautifulSoup(page.content)\n    #save data from one page into a dataframe\n    df = soup_to_df(soup)\n    # combine all data from all pages into one dataframe\n    df_all = pd.concat([df_all, df], axis = 0) \n```\n","source":"_posts/read_data.md","raw":"---\ntitle: Read data\n---\n### csv file\nUsing Pandas package  [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\", encoding = \"utf-8\") # check encoding type such like \"utf-16\"\n ```\n\n### data from website\nReading data about house price and house feature from a website as an example:\n\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef soup_to_df(s):\n    #define column name\n    dfcols = ['outcode', 'last_published_date','latitude', 'longitude', 'post_town', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n    df_xml = pd.DataFrame(columns=dfcols)\n\n    for node in s.find_all(\"listing\"):\n        outcode =  node.find('outcode').get_text()\n        last_published_date = node.find('last_published_date').get_text()\n        latitude = node.find('latitude').get_text()\n        longitude = node.find('longitude').get_text()\n        post_town = node.find('post_town').get_text()\n        num_bathrooms = node.find('num_bathrooms').get_text()\n        num_bedrooms = node.find('num_bedrooms').get_text()\n        num_floors = node.find('num_floors').get_text()\n        num_recepts = node.find('num_recepts').get_text()\n        property_type = node.find('property_type').get_text()\n        street_name = node.find('street_name').get_text()\n        price = node.find('price').get_text()\n\n        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),\n                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=True)   \n    return df_xml\n\ndfcols = ['outcode', 'latitude', 'longitude', 'post_town', 'last_published_date', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n\ndf_all = pd.DataFrame(columns=dfcols)\n\nfor i in range(1, 58): #page range\n# Ctrl+Shift+I in that website to find the url for which we would like to scrape. \n    baseurl = f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&county=Somerset&country=England&listing_status=sale&include_sold=1&page_number={i}&page_size=100\"\n    page = requests.get(baseurl)\n    soup = BeautifulSoup(page.content)\n    #save data from one page into a dataframe\n    df = soup_to_df(soup)\n    # combine all data from all pages into one dataframe\n    df_all = pd.concat([df_all, df], axis = 0) \n```\n","slug":"read_data","published":1,"date":"2019-07-25T03:31:57.387Z","updated":"2019-07-25T03:31:57.387Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbs0005c3dlxuvuscyj","content":"<h3 id=\"csv-file\"><a class=\"markdownIt-Anchor\" href=\"#csv-file\"></a> csv file</h3>\n<p>Using Pandas package  <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\" target=\"_blank\" rel=\"noopener\">read_csv</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">\"data.csv\"</span>, encoding = <span class=\"string\">\"utf-8\"</span>) <span class=\"comment\"># check encoding type such like \"utf-16\"</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"data-from-website\"><a class=\"markdownIt-Anchor\" href=\"#data-from-website\"></a> data from website</h3>\n<p>Reading data about house price and house feature from a website as an example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soup_to_df</span><span class=\"params\">(s)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\">#define column name</span></span><br><span class=\"line\">    dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'last_published_date'</span>,<span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\">    df_xml = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> node <span class=\"keyword\">in</span> s.find_all(<span class=\"string\">\"listing\"</span>):</span><br><span class=\"line\">        outcode =  node.find(<span class=\"string\">'outcode'</span>).get_text()</span><br><span class=\"line\">        last_published_date = node.find(<span class=\"string\">'last_published_date'</span>).get_text()</span><br><span class=\"line\">        latitude = node.find(<span class=\"string\">'latitude'</span>).get_text()</span><br><span class=\"line\">        longitude = node.find(<span class=\"string\">'longitude'</span>).get_text()</span><br><span class=\"line\">        post_town = node.find(<span class=\"string\">'post_town'</span>).get_text()</span><br><span class=\"line\">        num_bathrooms = node.find(<span class=\"string\">'num_bathrooms'</span>).get_text()</span><br><span class=\"line\">        num_bedrooms = node.find(<span class=\"string\">'num_bedrooms'</span>).get_text()</span><br><span class=\"line\">        num_floors = node.find(<span class=\"string\">'num_floors'</span>).get_text()</span><br><span class=\"line\">        num_recepts = node.find(<span class=\"string\">'num_recepts'</span>).get_text()</span><br><span class=\"line\">        property_type = node.find(<span class=\"string\">'property_type'</span>).get_text()</span><br><span class=\"line\">        street_name = node.find(<span class=\"string\">'street_name'</span>).get_text()</span><br><span class=\"line\">        price = node.find(<span class=\"string\">'price'</span>).get_text()</span><br><span class=\"line\"></span><br><span class=\"line\">        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),</span><br><span class=\"line\">                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=<span class=\"literal\">True</span>)   </span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_xml</span><br><span class=\"line\"></span><br><span class=\"line\">dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'last_published_date'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">df_all = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">58</span>): <span class=\"comment\">#page range</span></span><br><span class=\"line\"><span class=\"comment\"># Ctrl+Shift+I in that website to find the url for which we would like to scrape. </span></span><br><span class=\"line\">    baseurl = <span class=\"string\">f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&amp;county=Somerset&amp;country=England&amp;listing_status=sale&amp;include_sold=1&amp;page_number=<span class=\"subst\">&#123;i&#125;</span>&amp;page_size=100\"</span></span><br><span class=\"line\">    page = requests.get(baseurl)</span><br><span class=\"line\">    soup = BeautifulSoup(page.content)</span><br><span class=\"line\">    <span class=\"comment\">#save data from one page into a dataframe</span></span><br><span class=\"line\">    df = soup_to_df(soup)</span><br><span class=\"line\">    <span class=\"comment\"># combine all data from all pages into one dataframe</span></span><br><span class=\"line\">    df_all = pd.concat([df_all, df], axis = <span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"csv-file\"><a class=\"markdownIt-Anchor\" href=\"#csv-file\"></a> csv file</h3>\n<p>Using Pandas package  <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\" target=\"_blank\" rel=\"noopener\">read_csv</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">\"data.csv\"</span>, encoding = <span class=\"string\">\"utf-8\"</span>) <span class=\"comment\"># check encoding type such like \"utf-16\"</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"data-from-website\"><a class=\"markdownIt-Anchor\" href=\"#data-from-website\"></a> data from website</h3>\n<p>Reading data about house price and house feature from a website as an example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soup_to_df</span><span class=\"params\">(s)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\">#define column name</span></span><br><span class=\"line\">    dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'last_published_date'</span>,<span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\">    df_xml = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> node <span class=\"keyword\">in</span> s.find_all(<span class=\"string\">\"listing\"</span>):</span><br><span class=\"line\">        outcode =  node.find(<span class=\"string\">'outcode'</span>).get_text()</span><br><span class=\"line\">        last_published_date = node.find(<span class=\"string\">'last_published_date'</span>).get_text()</span><br><span class=\"line\">        latitude = node.find(<span class=\"string\">'latitude'</span>).get_text()</span><br><span class=\"line\">        longitude = node.find(<span class=\"string\">'longitude'</span>).get_text()</span><br><span class=\"line\">        post_town = node.find(<span class=\"string\">'post_town'</span>).get_text()</span><br><span class=\"line\">        num_bathrooms = node.find(<span class=\"string\">'num_bathrooms'</span>).get_text()</span><br><span class=\"line\">        num_bedrooms = node.find(<span class=\"string\">'num_bedrooms'</span>).get_text()</span><br><span class=\"line\">        num_floors = node.find(<span class=\"string\">'num_floors'</span>).get_text()</span><br><span class=\"line\">        num_recepts = node.find(<span class=\"string\">'num_recepts'</span>).get_text()</span><br><span class=\"line\">        property_type = node.find(<span class=\"string\">'property_type'</span>).get_text()</span><br><span class=\"line\">        street_name = node.find(<span class=\"string\">'street_name'</span>).get_text()</span><br><span class=\"line\">        price = node.find(<span class=\"string\">'price'</span>).get_text()</span><br><span class=\"line\"></span><br><span class=\"line\">        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),</span><br><span class=\"line\">                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=<span class=\"literal\">True</span>)   </span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_xml</span><br><span class=\"line\"></span><br><span class=\"line\">dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'last_published_date'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">df_all = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">58</span>): <span class=\"comment\">#page range</span></span><br><span class=\"line\"><span class=\"comment\"># Ctrl+Shift+I in that website to find the url for which we would like to scrape. </span></span><br><span class=\"line\">    baseurl = <span class=\"string\">f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&amp;county=Somerset&amp;country=England&amp;listing_status=sale&amp;include_sold=1&amp;page_number=<span class=\"subst\">&#123;i&#125;</span>&amp;page_size=100\"</span></span><br><span class=\"line\">    page = requests.get(baseurl)</span><br><span class=\"line\">    soup = BeautifulSoup(page.content)</span><br><span class=\"line\">    <span class=\"comment\">#save data from one page into a dataframe</span></span><br><span class=\"line\">    df = soup_to_df(soup)</span><br><span class=\"line\">    <span class=\"comment\"># combine all data from all pages into one dataframe</span></span><br><span class=\"line\">    df_all = pd.concat([df_all, df], axis = <span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n"},{"title":"Timeseries","date":"2019-07-25T06:51:27.000Z","_content":"---\n\n\n## ARIMA-autoregreesive integrated moving average\n\nBefore we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p > \\alpha, we say the process is not stationary at \\alpha significant level. \n\nIn the common time series model, we have autoregressive (AR) model--AR(p), moving average (MA) model--MA(q), autoregressive–moving-average (ARMA)--ARMA(p,q) and Autoregressive integrated moving average (ARIMA)--ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.  \n\n- AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)\n- I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1\n- MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.\n\nThe way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter. \n\nIn the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.\n\nTo find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.\n \n## Seasonal autoregressive integrated moving average model(SARIMA) \nSARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.\n\nThe general formula for SARIMA with seasonal period as 12(months per year) is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\"/>\n</p> \n\nwhere \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\"/>\n</p> \n\nSince we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.\n\nTo compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.\n\nSome comparison:\n\n| ARIMA model | mean square error |\n| ----------- | -------------- |\n| ARIMA(3,0,1)(2,1,2,12) |  275 |\n| ARIMA(3,1,2)(2,1,2,12) |  279 |\n| ARIMA(3,0,2)(0,1,2,12) |  260 |\n| ARIMA(3,0,2)(1,1,2,12) |  266 |\n| ARIMA(3,0,1)(1,1,2,12) |  276 |\n| ARIMA(2,0,1)(0,1,2,12) |  268 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(3,1,1)(0,1,2,12) |  270 |\n...\n\nThen ARIMA(3,0,2)(0,1,2,12) may be better here.\n","source":"_posts/timeseries.md","raw":"---\ntitle: Timeseries\ndate: 2019-07-25 14:51:27\ntags:\n---\n---\n\n\n## ARIMA-autoregreesive integrated moving average\n\nBefore we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p > \\alpha, we say the process is not stationary at \\alpha significant level. \n\nIn the common time series model, we have autoregressive (AR) model--AR(p), moving average (MA) model--MA(q), autoregressive–moving-average (ARMA)--ARMA(p,q) and Autoregressive integrated moving average (ARIMA)--ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.  \n\n- AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)\n- I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1\n- MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.\n\nThe way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter. \n\nIn the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.\n\nTo find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.\n \n## Seasonal autoregressive integrated moving average model(SARIMA) \nSARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.\n\nThe general formula for SARIMA with seasonal period as 12(months per year) is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\"/>\n</p> \n\nwhere \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\"/>\n</p> \n\nSince we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.\n\nTo compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.\n\nSome comparison:\n\n| ARIMA model | mean square error |\n| ----------- | -------------- |\n| ARIMA(3,0,1)(2,1,2,12) |  275 |\n| ARIMA(3,1,2)(2,1,2,12) |  279 |\n| ARIMA(3,0,2)(0,1,2,12) |  260 |\n| ARIMA(3,0,2)(1,1,2,12) |  266 |\n| ARIMA(3,0,1)(1,1,2,12) |  276 |\n| ARIMA(2,0,1)(0,1,2,12) |  268 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(3,1,1)(0,1,2,12) |  270 |\n...\n\nThen ARIMA(3,0,2)(0,1,2,12) may be better here.\n","slug":"timeseries","published":1,"updated":"2019-08-05T03:49:24.819Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzm0sbs0006c3dlzskn3ktj","content":"<hr>\n<h2 id=\"arima-autoregreesive-integrated-moving-average\"><a class=\"markdownIt-Anchor\" href=\"#arima-autoregreesive-integrated-moving-average\"></a> ARIMA-autoregreesive integrated moving average</h2>\n<p>Before we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p &gt; \\alpha, we say the process is not stationary at \\alpha significant level.</p>\n<p>In the common time series model, we have autoregressive (AR) model–AR§, moving average (MA) model–MA(q), autoregressive–moving-average (ARMA)–ARMA(p,q) and Autoregressive integrated moving average (ARIMA)–ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.</p>\n<ul>\n<li>AR: Auto-Regressive §: AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)</li>\n<li>I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1</li>\n<li>MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.</li>\n</ul>\n<p>The way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter.</p>\n<p>In the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.</p>\n<p>To find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.</p>\n<h2 id=\"seasonal-autoregressive-integrated-moving-average-modelsarima\"><a class=\"markdownIt-Anchor\" href=\"#seasonal-autoregressive-integrated-moving-average-modelsarima\"></a> Seasonal autoregressive integrated moving average model(SARIMA)</h2>\n<p>SARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.</p>\n<p>The general formula for SARIMA with seasonal period as 12(months per year) is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\">\n</p> \n<p>where</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\">\n</p> \n<p>Since we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.</p>\n<p>To compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.</p>\n<p>Some comparison:</p>\n<table>\n<thead>\n<tr>\n<th>ARIMA model</th>\n<th>mean square error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ARIMA(3,0,1)(2,1,2,12)</td>\n<td>275</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,2)(2,1,2,12)</td>\n<td>279</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(0,1,2,12)</td>\n<td>260</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(1,1,2,12)</td>\n<td>266</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,1)(1,1,2,12)</td>\n<td>276</td>\n</tr>\n<tr>\n<td>ARIMA(2,0,1)(0,1,2,12)</td>\n<td>268</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,1)(0,1,2,12)</td>\n<td>270</td>\n</tr>\n</tbody>\n</table>\n<p>…</p>\n<p>Then ARIMA(3,0,2)(0,1,2,12) may be better here.</p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h2 id=\"arima-autoregreesive-integrated-moving-average\"><a class=\"markdownIt-Anchor\" href=\"#arima-autoregreesive-integrated-moving-average\"></a> ARIMA-autoregreesive integrated moving average</h2>\n<p>Before we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p &gt; \\alpha, we say the process is not stationary at \\alpha significant level.</p>\n<p>In the common time series model, we have autoregressive (AR) model–AR§, moving average (MA) model–MA(q), autoregressive–moving-average (ARMA)–ARMA(p,q) and Autoregressive integrated moving average (ARIMA)–ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.</p>\n<ul>\n<li>AR: Auto-Regressive §: AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)</li>\n<li>I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1</li>\n<li>MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.</li>\n</ul>\n<p>The way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter.</p>\n<p>In the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.</p>\n<p>To find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.</p>\n<h2 id=\"seasonal-autoregressive-integrated-moving-average-modelsarima\"><a class=\"markdownIt-Anchor\" href=\"#seasonal-autoregressive-integrated-moving-average-modelsarima\"></a> Seasonal autoregressive integrated moving average model(SARIMA)</h2>\n<p>SARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.</p>\n<p>The general formula for SARIMA with seasonal period as 12(months per year) is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\">\n</p> \n<p>where</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\">\n</p> \n<p>Since we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.</p>\n<p>To compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.</p>\n<p>Some comparison:</p>\n<table>\n<thead>\n<tr>\n<th>ARIMA model</th>\n<th>mean square error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ARIMA(3,0,1)(2,1,2,12)</td>\n<td>275</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,2)(2,1,2,12)</td>\n<td>279</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(0,1,2,12)</td>\n<td>260</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(1,1,2,12)</td>\n<td>266</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,1)(1,1,2,12)</td>\n<td>276</td>\n</tr>\n<tr>\n<td>ARIMA(2,0,1)(0,1,2,12)</td>\n<td>268</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,1)(0,1,2,12)</td>\n<td>270</td>\n</tr>\n</tbody>\n</table>\n<p>…</p>\n<p>Then ARIMA(3,0,2)(0,1,2,12) may be better here.</p>\n"}],"PostAsset":[{"_id":"source/_posts/linear-model/orth.png","post":"cjyzm0sbp0002c3dl5h1hhwjy","slug":"orth.png","modified":1,"renderable":1}],"PostCategory":[],"PostTag":[],"Tag":[]}}