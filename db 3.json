{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/ocean/source/404.html","path":"404.html","modified":1,"renderable":1},{"_id":"themes/ocean/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/404.styl","path":"css/404.styl","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/busuanzi-2.3.pure.min.js","path":"js/busuanzi-2.3.pure.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/lazyload.min.js","path":"js/lazyload.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/ocean.js","path":"js/ocean.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/pace.min.js","path":"js/pace.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/hexo-inverted.svg","path":"images/hexo-inverted.svg","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/hexo.svg","path":"images/hexo.svg","modified":1,"renderable":1},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.eot","path":"css/feathericon/feathericon.eot","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.ttf","path":"css/feathericon/feathericon.ttf","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff","path":"css/feathericon/feathericon.woff","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff2","path":"css/feathericon/feathericon.woff2","modified":1,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.svg","path":"css/feathericon/feathericon.svg","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/overlay-hero.png","path":"images/ocean/overlay-hero.png","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/forrestgump.png","path":"images/forrestgump.png","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.ogv","path":"images/ocean/ocean.ogv","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/dz_hhh.jpeg","path":"images/ocean/dz_hhh.jpeg","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.png","path":"images/ocean/ocean.png","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.webm","path":"images/ocean/ocean.webm","modified":1,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.mp4","path":"images/ocean/ocean.mp4","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"cc75fbdb977a72e3c33a32b977ec965c1597d5c5","modified":1564025592527},{"_id":"themes/ocean/_config.yml","hash":"778d602eecf27a112b4fff0c10e298e743ce20e7","modified":1565073644799},{"_id":"themes/ocean/README.md","hash":"8e711610c3600376d55c60b28ab20ffa8157b060","modified":1564711687256},{"_id":"themes/ocean/package.json","hash":"b993176f8c35bc3ab9dbd8642ec6cd125fcb447e","modified":1564711687261},{"_id":"source/about/index.md","hash":"e22aaa74081b6582e04b77b2628266f1e6322eb6","modified":1565083719381},{"_id":"source/_posts/.DS_Store","hash":"d4a4a42252497c943da4150023ab9ad99174357b","modified":1564994868111},{"_id":"source/_posts/aic-bic.md","hash":"f74cc826b0802d26be23d982cb06e3e6949c94cc","modified":1565084984292},{"_id":"source/_posts/exp-family.md","hash":"ab83a690f3eb2aefbc8bfe4de5f1028fd4d79280","modified":1565257932607},{"_id":"source/_posts/feature_test.md","hash":"e15642b52d246f7c292bc8270ce133d7387e9bbf","modified":1565084950036},{"_id":"source/_posts/glm.md","hash":"f075ff5096ecd8ed8a0099eee1c6e4af528aadbb","modified":1565268032410},{"_id":"source/_posts/linear-model.md","hash":"9f276688127201976924fcba16dbcda3cba812f7","modified":1565258107674},{"_id":"source/_posts/machine_learning.md","hash":"9c1e48721590e2c5045e5fc2f0137f26784902e7","modified":1565084841176},{"_id":"source/_posts/read_data.md","hash":"e0b009d60b3f2e7f3f476692b3f346e112946715","modified":1565084683575},{"_id":"source/_posts/timeseries.md","hash":"4e54745c0a32894bcd6c84a2e97514074ef4ffbb","modified":1565084726672},{"_id":"source/gallery/index.md","hash":"4339ec60919504a722d48774fe00c66f7a3fcc65","modified":1565083416356},{"_id":"themes/ocean/.git/COMMIT_EDITMSG","hash":"f223f5f5f3aa2bf592883b4f3e203c9360c61574","modified":1565077185779},{"_id":"themes/ocean/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1564711687250},{"_id":"themes/ocean/.git/config","hash":"686588db57bb2d68a492446ce94aa90be25c28fa","modified":1564711687252},{"_id":"themes/ocean/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1564711566900},{"_id":"themes/ocean/.git/index","hash":"8ff1846e38c8432cf4e574d56f96651df05b908f","modified":1565077185778},{"_id":"themes/ocean/.git/packed-refs","hash":"d24bfad6d6e0ee981aec58816e07017ba2b00993","modified":1564711687249},{"_id":"themes/ocean/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1564711687256},{"_id":"themes/ocean/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1564711687256},{"_id":"themes/ocean/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1564711687257},{"_id":"themes/ocean/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1564711687257},{"_id":"themes/ocean/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1564711687257},{"_id":"themes/ocean/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1564711687257},{"_id":"themes/ocean/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1564711687257},{"_id":"themes/ocean/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1564711687257},{"_id":"themes/ocean/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1564711687257},{"_id":"themes/ocean/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1564711687257},{"_id":"themes/ocean/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1564711687257},{"_id":"themes/ocean/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1564711687257},{"_id":"themes/ocean/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1564711687260},{"_id":"themes/ocean/layout/categories.ejs","hash":"0034642381558bd76d973868d3828aea5bf3a8cd","modified":1564711687260},{"_id":"themes/ocean/layout/index.ejs","hash":"dead30ea8014348cef977dcb44eea0ae0f0601c5","modified":1564711687260},{"_id":"themes/ocean/layout/page.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1564711687260},{"_id":"themes/ocean/layout/layout.ejs","hash":"9ce598d82d973518e255fe64019b8523a2d65796","modified":1564711687260},{"_id":"themes/ocean/layout/post.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1564711687261},{"_id":"themes/ocean/layout/tags.ejs","hash":"77240e3266c91cc10e136abb03a7b076a1c45908","modified":1564711687261},{"_id":"themes/ocean/source/404.html","hash":"788929fab7b99dd74575399f41cddae6f63ce1f4","modified":1564711687263},{"_id":"themes/ocean/source/favicon.ico","hash":"0f20298a6a4d1ebd7a7ae7b87d7a3ae9afec0623","modified":1564711687269},{"_id":"themes/ocean/layout/_partial/<%- theme.ocean.path %>dz_hhh.png","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1565082555289},{"_id":"themes/ocean/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1564711566900},{"_id":"themes/ocean/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1564711566900},{"_id":"themes/ocean/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1564711566901},{"_id":"themes/ocean/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1564711566902},{"_id":"themes/ocean/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1564711566903},{"_id":"themes/ocean/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1564711566899},{"_id":"themes/ocean/.git/logs/HEAD","hash":"b3e43b700511ed845f55b38c794456308082bc60","modified":1565077185780},{"_id":"themes/ocean/layout/_partial/after-footer.ejs","hash":"8a33afc5b80613896f5a5912ff09be586af127be","modified":1565316981610},{"_id":"themes/ocean/layout/_partial/archive-post.ejs","hash":"9be7173badcca6582c1136204adb3aa432aada21","modified":1564711687258},{"_id":"themes/ocean/layout/_partial/archive.ejs","hash":"d7221ce7a6f5989ded47f7d9b0f40778f897deb6","modified":1564711687258},{"_id":"themes/ocean/layout/_partial/article.ejs","hash":"7d8bf7ed4cd9677a8edba59c47e16cdeef864aa5","modified":1565073474820},{"_id":"themes/ocean/layout/_partial/footer.ejs","hash":"47faf004b54c1ca278baf2230039af0c585dcda0","modified":1565329296530},{"_id":"themes/ocean/layout/_partial/head.ejs","hash":"ac644f55f1cacbf68c6347ca95b9aa87953afd31","modified":1565073813672},{"_id":"themes/ocean/layout/_partial/ocean.ejs","hash":"be76e0cbc4ecd9171972fabed6830cb592b5b343","modified":1565084284168},{"_id":"themes/ocean/layout/_partial/sidebar.ejs","hash":"5778e5f469ab9a3e17359080e2e1245a2082ec96","modified":1564711687260},{"_id":"themes/ocean/layout/_partial/totop.ejs","hash":"72b960315983ee95363fa9cabe82f52916ac9ae3","modified":1564711687260},{"_id":"themes/ocean/screenshots/hexo-theme-ocean.jpg","hash":"13b5045d2120cac2f68849757f5e0af08938b7c6","modified":1564711687263},{"_id":"themes/ocean/source/css/404.styl","hash":"57a7cd3fb84232a34687963ae36c0e469e5918a3","modified":1564711687263},{"_id":"themes/ocean/source/css/_extend.styl","hash":"deb6aca91c40516f5d638008a72f9def42e5d081","modified":1564711687263},{"_id":"themes/ocean/source/css/_feathericon.styl","hash":"8494f0e869411781264868f08eda62fd838e0cee","modified":1564711687263},{"_id":"themes/ocean/source/css/_mixins.styl","hash":"6959409df2dd0a1ca05be0c0e9b2a884efdfb82d","modified":1564711687263},{"_id":"themes/ocean/source/css/_normalize.styl","hash":"b3337320133b7a336db7033aa6bbe94b054c0b21","modified":1564711687263},{"_id":"themes/ocean/source/css/_variables.styl","hash":"bc03c1fdc8c1d8568c506a7ca3aa149dbde02e04","modified":1564711687266},{"_id":"themes/ocean/source/css/style.styl","hash":"8dbe0f34f7e9a20b7b6fc5cc1458ad7931510ff5","modified":1564711687268},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1564711687268},{"_id":"themes/ocean/source/js/busuanzi-2.3.pure.min.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1564711687301},{"_id":"themes/ocean/source/js/lazyload.min.js","hash":"b801b3946fb9b72e03512c0663458e140e1fa77b","modified":1564711687302},{"_id":"themes/ocean/source/js/ocean.js","hash":"3457be62843930ad58997cd6fd387783285242c7","modified":1564711687302},{"_id":"themes/ocean/source/js/pace.min.js","hash":"d32ab818e0f97d3b0c80f5631fc23d8a0cb52795","modified":1564711687302},{"_id":"themes/ocean/source/js/search.js","hash":"118be0e0918532ac1225f62e1a0a6f0673e0b173","modified":1564711687302},{"_id":"themes/ocean/source/images/hexo-inverted.svg","hash":"525309ea3c7360f83d1d9df6d04c256d7171950d","modified":1564711687270},{"_id":"themes/ocean/source/images/hexo.svg","hash":"71e7204d04ccfe260f06ea5873484791cd5f404a","modified":1564711687270},{"_id":"source/_posts/linear-model/orth.png","hash":"c4eef4c96331fa979c9d773ac2a84557a3204d43","modified":1564995664206},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.js","hash":"b2b093d8f5ffeee250c8d0d3a2285a213318e4ea","modified":1564711687269},{"_id":"themes/ocean/source/js/jquery-2.0.3.min.js","hash":"800edb7787c30f4982bf38f2cb8f4f6fb61340e9","modified":1564711687301},{"_id":"themes/ocean/.git/objects/23/b90c59e258d0c33d67412a58b9bacaf019795a","hash":"d438f0c29bb67859d6ec0ccafff8c41d7c043da7","modified":1565077160794},{"_id":"themes/ocean/.git/objects/28/a966d41eef6fe849db5df61eb212bdc5a9419d","hash":"2828f3a580abaa4d99462c63c8bd298d8dc045f3","modified":1565077185779},{"_id":"themes/ocean/.git/objects/2c/c06299dec23c7d4cd2ca478c538671a957a198","hash":"02f51387a5df77800b00c3005dd9a3a8d95df8f2","modified":1565077160792},{"_id":"themes/ocean/.git/objects/5a/f8fcec5a9fa5028ab7585feefae6ced4c24ce6","hash":"af14ed8f80caba748d5a287930a07b659ce2eecd","modified":1565077185776},{"_id":"themes/ocean/.git/objects/60/f27e0cab07076f9f11c69becfed8cf1e84abe4","hash":"f67edf9a86a3ec19f79f406d0a7aca0ef798268e","modified":1565077160794},{"_id":"themes/ocean/.git/objects/99/009d068ee135465f6883e3675d467ac918d0e6","hash":"b114f765b7457c8516f88aac3303f782ce64899b","modified":1565077185778},{"_id":"themes/ocean/.git/objects/b5/a8368f3a2ca6b6d9c85e2cf05f5b595ea70278","hash":"b1a06e8a3519d51f5bd2ca4536e6e048893e50d1","modified":1565077185777},{"_id":"themes/ocean/.git/objects/f4/79507539496dabe1dfec2db08fd02ac2736dd4","hash":"ce76629ecafaa79c2b90f3f42036230249a55867","modified":1565077185777},{"_id":"themes/ocean/.git/objects/f6/e543fca3dd12ed8c47bc201a32774ea95c90f4","hash":"64d8f57984f2440cb16217876d42a90815c719b4","modified":1565077160793},{"_id":"themes/ocean/.git/objects/fa/20d5673033e2431d4fe945fc27f259deb191c7","hash":"1957dd04412716092a3187f6974a5709bb8c55a8","modified":1565077160793},{"_id":"themes/ocean/.git/objects/pack/pack-9cc67a3a009ce4a7eab3bf2787338d141b31feb7.idx","hash":"6ff7947e4bb9987fca29b8a6b030d6523c5aa356","modified":1564711687236},{"_id":"themes/ocean/.git/refs/heads/master","hash":"ec5387aa03acf5bd4623e046c6616417b3c28ef8","modified":1565077185780},{"_id":"themes/ocean/layout/_partial/post/albums.ejs","hash":"26812d13f8b7edda90c0603aa1bb7a592ee2deed","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/busuanzi.ejs","hash":"812617702a3cc8c981ef7d96fcea9e89afd1157d","modified":1565329539246},{"_id":"themes/ocean/layout/_partial/post/category.ejs","hash":"85f0ebeceee1c32623bfa1e4170dbe1e34442fea","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/gallery.ejs","hash":"5f8487fe7bed9a09001c6655244ff35f583cf1eb","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/gitalk.ejs","hash":"e36d149ad83c3a52562dbef61a0083957eb24578","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/nav.ejs","hash":"e59198918e92ef92156aeefbf6023584ac1cae64","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/search.ejs","hash":"2c9d19d1685e834aa2020998da2a2d259ce9b9ff","modified":1564711687259},{"_id":"themes/ocean/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1564711687260},{"_id":"themes/ocean/layout/_partial/post/title.ejs","hash":"53ccbfc6f1c424fb4dd609c1a61ffb69841403cc","modified":1564711687260},{"_id":"themes/ocean/layout/_partial/post/topping.ejs","hash":"bacd7e1d09397cfb32d97b5f3296f3ac538e57ea","modified":1564711687260},{"_id":"themes/ocean/source/css/_partial/albums.styl","hash":"0659d5f7469f24a415354ff767d949926465d515","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/apple.styl","hash":"e06dce604cc58ec39d677e4e59910c2725684901","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/archive.styl","hash":"8aefdcf2d542ad839018c2c58511e3318a38490d","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/article.styl","hash":"4c4ab4166987abe8a4e3df5bec11ec072f35a3ea","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/articles.styl","hash":"7bf289013d304505984b251be725b49165a694fd","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/categories.styl","hash":"f0c898823a5ddc37ae6bf76cc34ce8e50dd30885","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/float.styl","hash":"d888df89a172e4c8119cb8740fc1eae1a9539157","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/footer.styl","hash":"24779cbce1012d4f35ffc6b3ec0830cbc2ea3b3f","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/gallery.styl","hash":"7bdc2c9fb4971dbd7511c5cbb69bd611f20db591","modified":1564711687264},{"_id":"themes/ocean/source/css/_partial/gitalk.styl","hash":"3706eef2e0541493f1679a30241d279e29dfdc17","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/highlight.styl","hash":"c6e99fd23056fb01177aeefbc5dd4a8e88cf8f81","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/lists.styl","hash":"087f08e0ce9aca48e096dabca6eed2368b5bcd6b","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/layou.styl","hash":"445cccbdf3a01c340688f28124ac156b0b6d0214","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/mobile.styl","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/navbar.styl","hash":"4b41a8cde18b10d6e5f1a2adab0f4d3e81869b7e","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/ocean.styl","hash":"69ba351909c73eb1e04510facc9b35dd584198e0","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/pace.styl","hash":"e326918ba276ee332d0598d8193ccd8353e7d916","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/sidebar.styl","hash":"600c70f1de82da5223af290d47a583f9c379d188","modified":1564711687266},{"_id":"themes/ocean/source/css/_partial/search.styl","hash":"011aaf21942dfff514ed4e98ce20142efbdd1b71","modified":1564711687265},{"_id":"themes/ocean/source/css/_partial/tag.styl","hash":"925af8beede44ab53fe3cd0a5c472d2baa03baec","modified":1564711687266},{"_id":"themes/ocean/source/css/_partial/totop.styl","hash":"4bae031b6852384666cdf36e98c6bbbba1281453","modified":1564711687266},{"_id":"themes/ocean/source/css/feathericon/feathericon.eot","hash":"e2a01ae6f849841bc7a9fd21e5b7b450f1ded19b","modified":1564711687266},{"_id":"themes/ocean/source/css/feathericon/feathericon.ttf","hash":"d0d80c3c960d7d45e6bd7fa428d8a6a8c8245b2d","modified":1564711687267},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff","hash":"d22fe861e47afd92969ab46c7cbb7ea9c225aaf8","modified":1564711687268},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff2","hash":"2c11c45331d914ee38ad42ccf966132a508b5596","modified":1564711687268},{"_id":"themes/ocean/source/css/feathericon/feathericon.svg","hash":"c113006c6822451802c8457128c352c0e4934453","modified":1564711687267},{"_id":"themes/ocean/source/images/ocean/overlay-hero.png","hash":"92481a1848c35be96a693af11f77265323a7c189","modified":1564711687301},{"_id":"themes/ocean/.git/logs/refs/heads/master","hash":"b3e43b700511ed845f55b38c794456308082bc60","modified":1565077185780},{"_id":"themes/ocean/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1564711687250},{"_id":"themes/ocean/source/images/forrestgump.png","hash":"18ad6a8ba815878e36a0d5562136dc4fb8920c12","modified":1564711687270},{"_id":"themes/ocean/.git/logs/refs/remotes/origin/HEAD","hash":"add0d37464a85f3b7bc22cd37a8df8a31c03eb25","modified":1564711687250},{"_id":"themes/ocean/source/images/ocean/ocean.ogv","hash":"9c6b5d6b0544472cee39f5eafac2d5cbba5fd86b","modified":1564711687287},{"_id":"themes/ocean/source/images/ocean/dz_hhh.jpeg","hash":"dd9dcdd26b4c6a0dcce182a5a730e184a9ccef61","modified":1565078323580},{"_id":"themes/ocean/source/images/ocean/ocean.png","hash":"8245d07f812625d19b48ad2d00f8191f2aa4d304","modified":1564711687291},{"_id":"themes/ocean/source/images/ocean/ocean.webm","hash":"65aa2b6483e0151611899e31571057334c60d9e4","modified":1564711687300},{"_id":"themes/ocean/source/images/ocean/ocean.mp4","hash":"1e89cac2d652005d9dafd3ecb4dd460a8ff6d6af","modified":1564711687283},{"_id":"themes/ocean/.git/objects/pack/pack-9cc67a3a009ce4a7eab3bf2787338d141b31feb7.pack","hash":"95907dbd7f581b62a8b5eda05c0975e2b50948a0","modified":1564711687235}],"Category":[],"Data":[],"Page":[{"title":"about","date":"2019-08-06T09:21:40.000Z","_content":"123123123","source":"about/index.md","raw":"---\ntitle: about\ndate: 2019-08-06 17:21:40\n---\n123123123","updated":"2019-08-06T09:28:39.381Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjz3ososl0000p1dlhoizbex5","content":"<p>123123123</p>\n","site":{"data":{}},"excerpt":"","more":"<p>123123123</p>\n"},{"title":"gallery","date":"2019-08-06T09:20:58.000Z","albums":[["img_url","img_caption"],["img_url","img_caption"]],"_content":"","source":"gallery/index.md","raw":"---\ntitle: gallery\ndate: 2019-08-06 17:20:58\nalbums: [\n        [\"img_url\",\"img_caption\"],\n        [\"img_url\",\"img_caption\"]\n        ]\n---\n","updated":"2019-08-06T09:23:36.356Z","path":"gallery/index.html","comments":1,"layout":"page","_id":"cjz3osotg0009p1dlp2gmry2f","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"AIC and BIC","date":"2019-07-29T03:51:17.000Z","_content":"Introduction 2 model estimation AIC and BIC.\n<!--more-->\n\n## AIC \n\n\n## Note \nAs you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form \n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\"/>\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\"/>\n</p>\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\"/>\n</p>\n\n\nwhere $f(x | \\theta)$ is joint probability density function, $k$ is the number of estimated parameter in our model.\n\nFor AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.\n\nAIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.\n\nAIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.\n\nSo what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.\n\n","source":"_posts/aic-bic.md","raw":"---\ntitle: AIC and BIC\ndate: 2019-07-29 11:51:17\ntags:\n---\nIntroduction 2 model estimation AIC and BIC.\n<!--more-->\n\n## AIC \n\n\n## Note \nAs you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form \n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\"/>\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\"/>\n</p>\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.\n\nwhere $\\hat{L}$ is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\"/>\n</p>\n\n\nwhere $f(x | \\theta)$ is joint probability density function, $k$ is the number of estimated parameter in our model.\n\nFor AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.\n\nAIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.\n\nAIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.\n\nSo what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.\n\n","slug":"aic-bic","published":1,"updated":"2019-08-06T09:49:44.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3ososm0001p1dlrxhxfw6c","content":"<p>Introduction 2 model estimation AIC and BIC.</p>\n<a id=\"more\"></a>\n<h2 id=\"aic\"><a class=\"markdownIt-Anchor\" href=\"#aic\"></a> AIC</h2>\n<h2 id=\"note\"><a class=\"markdownIt-Anchor\" href=\"#note\"></a> Note</h2>\n<p>As you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form</p>\n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\">\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x | \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is joint probability density function, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> is the number of estimated parameter in our model.</p>\n<p>For AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.</p>\n<p>AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.</p>\n<p>AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.</p>\n<p>So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.</p>\n","site":{"data":{}},"excerpt":"<p>Introduction 2 model estimation AIC and BIC.</p>","more":"<h2 id=\"aic\"><a class=\"markdownIt-Anchor\" href=\"#aic\"></a> AIC</h2>\n<h2 id=\"note\"><a class=\"markdownIt-Anchor\" href=\"#note\"></a> Note</h2>\n<p>As you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form</p>\n<div style=\"text-align:center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;AIC=-2ln(\\hat{L}) + 2k\">\n</div>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;BIC=-2ln(\\hat{L}) + ln(n)k\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{L}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">^</span></span></span></span></span></span></span></span></span> is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;L(\\theta | x) = f(x | \\theta)\">\n</p>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x | \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is joint probability density function, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> is the number of estimated parameter in our model.</p>\n<p>For AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.</p>\n<p>AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.</p>\n<p>AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.</p>\n<p>So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.</p>"},{"title":"The exponential family","date":"2019-08-06T03:21:35.000Z","math":true,"_content":"Introduction of exponential family\n<!--more-->\n# The exponential family\n\n$p(y;\\eta) = b(y) exp(\\eta^T T(y) - a(\\eta))$\n\nwhere\n* $\\eta$ is natural parameter\n* $T(y)$ is sufficient statistic\n* $a(\\eta)$ is log partition function\n\nIf $T(y) = y$, then we want to know $E(y|x)$. Hence the expected outcome $y=E(y|x)=h(x)$ \n\n\n$$\n  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n$$","source":"_posts/exp-family.md","raw":"---\ntitle: The exponential family\ndate: 2019-08-06 11:21:35\ntags:\nmath: true\n---\nIntroduction of exponential family\n<!--more-->\n# The exponential family\n\n$p(y;\\eta) = b(y) exp(\\eta^T T(y) - a(\\eta))$\n\nwhere\n* $\\eta$ is natural parameter\n* $T(y)$ is sufficient statistic\n* $a(\\eta)$ is log partition function\n\nIf $T(y) = y$, then we want to know $E(y|x)$. Hence the expected outcome $y=E(y|x)=h(x)$ \n\n\n$$\n  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n$$","slug":"exp-family","published":1,"updated":"2019-08-08T09:52:12.607Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3ososp0002p1dl0kvor7qy","content":"<p>Introduction of exponential family</p>\n<a id=\"more\"></a>\n<h1 id=\"the-exponential-family\"><a class=\"markdownIt-Anchor\" href=\"#the-exponential-family\"></a> The exponential family</h1>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo separator=\"true\">;</mo><mi>η</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>b</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msup><mi>η</mi><mi>T</mi></msup><mi>T</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>a</mi><mo stretchy=\"false\">(</mo><mi>η</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(y;\\eta) = b(y) exp(\\eta^T T(y) - a(\\eta))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">x</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></p>\n<p>where</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span></span></span></span> is natural parameter</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">T(y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span> is sufficient statistic</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><mo stretchy=\"false\">(</mo><mi>η</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">a(\\eta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mclose\">)</span></span></span></span> is log partition function</li>\n</ul>\n<p>If <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">T(y) = y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, then we want to know <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E(y|x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span>. Hence the expected outcome <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>E</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">y=E(y|x)=h(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">h</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span></p>\n<p class=\"katex-block katex-error\" title=\"ParseError: KaTeX parse error: Undefined control sequence: \\lnp at position 18: …max\\sum_{i=1}^l\\̲l̲n̲p̲(x_{i};\\theta)\n\">  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n</p>\n","site":{"data":{}},"excerpt":"<p>Introduction of exponential family</p>","more":"<h1 id=\"the-exponential-family\"><a class=\"markdownIt-Anchor\" href=\"#the-exponential-family\"></a> The exponential family</h1>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo separator=\"true\">;</mo><mi>η</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>b</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msup><mi>η</mi><mi>T</mi></msup><mi>T</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>a</mi><mo stretchy=\"false\">(</mo><mi>η</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(y;\\eta) = b(y) exp(\\eta^T T(y) - a(\\eta))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">x</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></p>\n<p>where</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span></span></span></span> is natural parameter</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">T(y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span> is sufficient statistic</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><mo stretchy=\"false\">(</mo><mi>η</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">a(\\eta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">a</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mclose\">)</span></span></span></span> is log partition function</li>\n</ul>\n<p>If <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">T(y) = y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, then we want to know <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E(y|x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span>. Hence the expected outcome <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>E</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">y=E(y|x)=h(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">h</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span></p>\n<p class=\"katex-block katex-error\" title=\"ParseError: KaTeX parse error: Undefined control sequence: \\lnp at position 18: …max\\sum_{i=1}^l\\̲l̲n̲p̲(x_{i};\\theta)\n\">  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n</p>"},{"title":"Find important feature","_content":"Introduction of feature engineering. Include some method such like p-value and F-test in Pyhton.\n<!--more-->\n\n## Select feature\n### Reason to do feature selection\n1. Because a large number of features may cost long training time\n2. Increasing number of features may increase the risk of overfitting\nIt can also help us to reduce the dimension of our dataset without loss main information.\n### Methods of feature selection\n#### P-value\nP-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.\n```python\nimport statsmodels.api as sm\n#Adding constant column of ones, mandatory for sm.OLS model\nx = pd.DataFrame() # feature df\ny = pd.DataFrame() # target df\nx_1 = sm.add_constant(x) # add a constant column to df x\n#Fitting sm.OLS model\nmodel = sm.OLS(np.array(y), np.array(x_1)).fit()\n# Which prints out the p-value of each features in this model as an array \nmodel.pvalues\n```\n\nWrite a function to select feature based on p-value:\n```python\n# x is feature df\n# y is target df\n# sl is significant level\ndef backwardElimination(x, y, sl):\n    cols = list(x.columns)\n    pmax = 1\n    while (len(cols) > 0):\n        p= []                                                                                   \n        X_1 = x[cols]\n        X_1 = sm.add_constant(X_1)\n        model = sm.OLS(y, X_1).fit()\n        p = pd.Series(model.pvalues.values[1:], index = cols)      \n        pmax = max(p)\n        # .idxmax returns the index of maximum\n        feature_with_p_max = p.idxmax()\n        if(pmax > sl):\n            cols.remove(feature_with_p_max)\n        else:\n            break\n    selected_features_BE = cols\n    print(selected_features_BE)\n\nbackwardElimination(x, y, 0.05)\n```\n\n\n#### F-test\nF-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.\n\nHere introduced the [skitlearn package](https://scikit-learn.org/stable/), we will use [F-test](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) to find first K best features:\n\nFor continues data type\n```python\nfrom sklearn import sklearn.feature_selection\nsklearn.feature_selection.f_regression(x, y) # where x is feature df(n_sample * n_features), y is target df (n_samples)\n# output is set of F-score and p-value for each F-score\n```\nFor classification data\n```python\nsklearn.feature_selection.f_classif(x, y) # same with f_regression\nsklearn.feature_selection.chi2(x, y) # if x is sparse, then only use chi2 can still keep it sparsity. \n```\nF-score is good for linear relation\n\n#### Mutual infomation\nIf x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.\nMore detail in [sklearn Mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\nWhich is good for non-linear relation\n```python\n# x is feature df, y is target df\n# For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X\n# n_neighbor higher values reduce variance of the estimation\nsklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=3, copy=True, random_state=None)\nsklearn.feature_selection.mututal_info_classif(x, y)\n\n# output is estimated MI between each feature and target\n```\n#### Variance threshold\nWhich is only care about feature itself: if it not vary a lot, then it has poor predictive power.\n```python\nsklearn.feature_selection.VarianceThreshold\n```\n\n","source":"_posts/feature_test.md","raw":"---\ntitle: Find important feature\n---\nIntroduction of feature engineering. Include some method such like p-value and F-test in Pyhton.\n<!--more-->\n\n## Select feature\n### Reason to do feature selection\n1. Because a large number of features may cost long training time\n2. Increasing number of features may increase the risk of overfitting\nIt can also help us to reduce the dimension of our dataset without loss main information.\n### Methods of feature selection\n#### P-value\nP-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.\n```python\nimport statsmodels.api as sm\n#Adding constant column of ones, mandatory for sm.OLS model\nx = pd.DataFrame() # feature df\ny = pd.DataFrame() # target df\nx_1 = sm.add_constant(x) # add a constant column to df x\n#Fitting sm.OLS model\nmodel = sm.OLS(np.array(y), np.array(x_1)).fit()\n# Which prints out the p-value of each features in this model as an array \nmodel.pvalues\n```\n\nWrite a function to select feature based on p-value:\n```python\n# x is feature df\n# y is target df\n# sl is significant level\ndef backwardElimination(x, y, sl):\n    cols = list(x.columns)\n    pmax = 1\n    while (len(cols) > 0):\n        p= []                                                                                   \n        X_1 = x[cols]\n        X_1 = sm.add_constant(X_1)\n        model = sm.OLS(y, X_1).fit()\n        p = pd.Series(model.pvalues.values[1:], index = cols)      \n        pmax = max(p)\n        # .idxmax returns the index of maximum\n        feature_with_p_max = p.idxmax()\n        if(pmax > sl):\n            cols.remove(feature_with_p_max)\n        else:\n            break\n    selected_features_BE = cols\n    print(selected_features_BE)\n\nbackwardElimination(x, y, 0.05)\n```\n\n\n#### F-test\nF-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.\n\nHere introduced the [skitlearn package](https://scikit-learn.org/stable/), we will use [F-test](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) to find first K best features:\n\nFor continues data type\n```python\nfrom sklearn import sklearn.feature_selection\nsklearn.feature_selection.f_regression(x, y) # where x is feature df(n_sample * n_features), y is target df (n_samples)\n# output is set of F-score and p-value for each F-score\n```\nFor classification data\n```python\nsklearn.feature_selection.f_classif(x, y) # same with f_regression\nsklearn.feature_selection.chi2(x, y) # if x is sparse, then only use chi2 can still keep it sparsity. \n```\nF-score is good for linear relation\n\n#### Mutual infomation\nIf x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.\nMore detail in [sklearn Mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\nWhich is good for non-linear relation\n```python\n# x is feature df, y is target df\n# For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X\n# n_neighbor higher values reduce variance of the estimation\nsklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=3, copy=True, random_state=None)\nsklearn.feature_selection.mututal_info_classif(x, y)\n\n# output is estimated MI between each feature and target\n```\n#### Variance threshold\nWhich is only care about feature itself: if it not vary a lot, then it has poor predictive power.\n```python\nsklearn.feature_selection.VarianceThreshold\n```\n\n","slug":"feature_test","published":1,"date":"2019-07-25T03:31:57.387Z","updated":"2019-08-06T09:49:10.036Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3ososr0003p1dl7zf4aq6g","content":"<p>Introduction of feature engineering. Include some method such like p-value and F-test in Pyhton.</p>\n<a id=\"more\"></a>\n<h2 id=\"select-feature\"><a class=\"markdownIt-Anchor\" href=\"#select-feature\"></a> Select feature</h2>\n<h3 id=\"reason-to-do-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#reason-to-do-feature-selection\"></a> Reason to do feature selection</h3>\n<ol>\n<li>Because a large number of features may cost long training time</li>\n<li>Increasing number of features may increase the risk of overfitting<br>\nIt can also help us to reduce the dimension of our dataset without loss main information.</li>\n</ol>\n<h3 id=\"methods-of-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#methods-of-feature-selection\"></a> Methods of feature selection</h3>\n<h4 id=\"p-value\"><a class=\"markdownIt-Anchor\" href=\"#p-value\"></a> P-value</h4>\n<p>P-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> statsmodels.api <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"><span class=\"comment\">#Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class=\"line\">x = pd.DataFrame() <span class=\"comment\"># feature df</span></span><br><span class=\"line\">y = pd.DataFrame() <span class=\"comment\"># target df</span></span><br><span class=\"line\">x_1 = sm.add_constant(x) <span class=\"comment\"># add a constant column to df x</span></span><br><span class=\"line\"><span class=\"comment\">#Fitting sm.OLS model</span></span><br><span class=\"line\">model = sm.OLS(np.array(y), np.array(x_1)).fit()</span><br><span class=\"line\"><span class=\"comment\"># Which prints out the p-value of each features in this model as an array </span></span><br><span class=\"line\">model.pvalues</span><br></pre></td></tr></table></figure>\n<p>Write a function to select feature based on p-value:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df</span></span><br><span class=\"line\"><span class=\"comment\"># y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># sl is significant level</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backwardElimination</span><span class=\"params\">(x, y, sl)</span>:</span></span><br><span class=\"line\">    cols = list(x.columns)</span><br><span class=\"line\">    pmax = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (len(cols) &gt; <span class=\"number\">0</span>):</span><br><span class=\"line\">        p= []                                                                                   </span><br><span class=\"line\">        X_1 = x[cols]</span><br><span class=\"line\">        X_1 = sm.add_constant(X_1)</span><br><span class=\"line\">        model = sm.OLS(y, X_1).fit()</span><br><span class=\"line\">        p = pd.Series(model.pvalues.values[<span class=\"number\">1</span>:], index = cols)      </span><br><span class=\"line\">        pmax = max(p)</span><br><span class=\"line\">        <span class=\"comment\"># .idxmax returns the index of maximum</span></span><br><span class=\"line\">        feature_with_p_max = p.idxmax()</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pmax &gt; sl):</span><br><span class=\"line\">            cols.remove(feature_with_p_max)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    selected_features_BE = cols</span><br><span class=\"line\">    print(selected_features_BE)</span><br><span class=\"line\"></span><br><span class=\"line\">backwardElimination(x, y, <span class=\"number\">0.05</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"f-test\"><a class=\"markdownIt-Anchor\" href=\"#f-test\"></a> F-test</h4>\n<p>F-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.</p>\n<p>Here introduced the <a href=\"https://scikit-learn.org/stable/\" target=\"_blank\" rel=\"noopener\">skitlearn package</a>, we will use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html\" target=\"_blank\" rel=\"noopener\">F-test</a> to find first K best features:</p>\n<p>For continues data type</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> sklearn.feature_selection</span><br><span class=\"line\">sklearn.feature_selection.f_regression(x, y) <span class=\"comment\"># where x is feature df(n_sample * n_features), y is target df (n_samples)</span></span><br><span class=\"line\"><span class=\"comment\"># output is set of F-score and p-value for each F-score</span></span><br></pre></td></tr></table></figure>\n<p>For classification data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.f_classif(x, y) <span class=\"comment\"># same with f_regression</span></span><br><span class=\"line\">sklearn.feature_selection.chi2(x, y) <span class=\"comment\"># if x is sparse, then only use chi2 can still keep it sparsity.</span></span><br></pre></td></tr></table></figure>\n<p>F-score is good for linear relation</p>\n<h4 id=\"mutual-infomation\"><a class=\"markdownIt-Anchor\" href=\"#mutual-infomation\"></a> Mutual infomation</h4>\n<p>If x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.<br>\nMore detail in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression\" target=\"_blank\" rel=\"noopener\">sklearn Mutual information</a><br>\nWhich is good for non-linear relation</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df, y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X</span></span><br><span class=\"line\"><span class=\"comment\"># n_neighbor higher values reduce variance of the estimation</span></span><br><span class=\"line\">sklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=<span class=\"number\">3</span>, copy=<span class=\"literal\">True</span>, random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\">sklearn.feature_selection.mututal_info_classif(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output is estimated MI between each feature and target</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"variance-threshold\"><a class=\"markdownIt-Anchor\" href=\"#variance-threshold\"></a> Variance threshold</h4>\n<p>Which is only care about feature itself: if it not vary a lot, then it has poor predictive power.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>Introduction of feature engineering. Include some method such like p-value and F-test in Pyhton.</p>","more":"<h2 id=\"select-feature\"><a class=\"markdownIt-Anchor\" href=\"#select-feature\"></a> Select feature</h2>\n<h3 id=\"reason-to-do-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#reason-to-do-feature-selection\"></a> Reason to do feature selection</h3>\n<ol>\n<li>Because a large number of features may cost long training time</li>\n<li>Increasing number of features may increase the risk of overfitting<br>\nIt can also help us to reduce the dimension of our dataset without loss main information.</li>\n</ol>\n<h3 id=\"methods-of-feature-selection\"><a class=\"markdownIt-Anchor\" href=\"#methods-of-feature-selection\"></a> Methods of feature selection</h3>\n<h4 id=\"p-value\"><a class=\"markdownIt-Anchor\" href=\"#p-value\"></a> P-value</h4>\n<p>P-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> statsmodels.api <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"><span class=\"comment\">#Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class=\"line\">x = pd.DataFrame() <span class=\"comment\"># feature df</span></span><br><span class=\"line\">y = pd.DataFrame() <span class=\"comment\"># target df</span></span><br><span class=\"line\">x_1 = sm.add_constant(x) <span class=\"comment\"># add a constant column to df x</span></span><br><span class=\"line\"><span class=\"comment\">#Fitting sm.OLS model</span></span><br><span class=\"line\">model = sm.OLS(np.array(y), np.array(x_1)).fit()</span><br><span class=\"line\"><span class=\"comment\"># Which prints out the p-value of each features in this model as an array </span></span><br><span class=\"line\">model.pvalues</span><br></pre></td></tr></table></figure>\n<p>Write a function to select feature based on p-value:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df</span></span><br><span class=\"line\"><span class=\"comment\"># y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># sl is significant level</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backwardElimination</span><span class=\"params\">(x, y, sl)</span>:</span></span><br><span class=\"line\">    cols = list(x.columns)</span><br><span class=\"line\">    pmax = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (len(cols) &gt; <span class=\"number\">0</span>):</span><br><span class=\"line\">        p= []                                                                                   </span><br><span class=\"line\">        X_1 = x[cols]</span><br><span class=\"line\">        X_1 = sm.add_constant(X_1)</span><br><span class=\"line\">        model = sm.OLS(y, X_1).fit()</span><br><span class=\"line\">        p = pd.Series(model.pvalues.values[<span class=\"number\">1</span>:], index = cols)      </span><br><span class=\"line\">        pmax = max(p)</span><br><span class=\"line\">        <span class=\"comment\"># .idxmax returns the index of maximum</span></span><br><span class=\"line\">        feature_with_p_max = p.idxmax()</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pmax &gt; sl):</span><br><span class=\"line\">            cols.remove(feature_with_p_max)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    selected_features_BE = cols</span><br><span class=\"line\">    print(selected_features_BE)</span><br><span class=\"line\"></span><br><span class=\"line\">backwardElimination(x, y, <span class=\"number\">0.05</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"f-test\"><a class=\"markdownIt-Anchor\" href=\"#f-test\"></a> F-test</h4>\n<p>F-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.</p>\n<p>Here introduced the <a href=\"https://scikit-learn.org/stable/\" target=\"_blank\" rel=\"noopener\">skitlearn package</a>, we will use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html\" target=\"_blank\" rel=\"noopener\">F-test</a> to find first K best features:</p>\n<p>For continues data type</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> sklearn.feature_selection</span><br><span class=\"line\">sklearn.feature_selection.f_regression(x, y) <span class=\"comment\"># where x is feature df(n_sample * n_features), y is target df (n_samples)</span></span><br><span class=\"line\"><span class=\"comment\"># output is set of F-score and p-value for each F-score</span></span><br></pre></td></tr></table></figure>\n<p>For classification data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.f_classif(x, y) <span class=\"comment\"># same with f_regression</span></span><br><span class=\"line\">sklearn.feature_selection.chi2(x, y) <span class=\"comment\"># if x is sparse, then only use chi2 can still keep it sparsity.</span></span><br></pre></td></tr></table></figure>\n<p>F-score is good for linear relation</p>\n<h4 id=\"mutual-infomation\"><a class=\"markdownIt-Anchor\" href=\"#mutual-infomation\"></a> Mutual infomation</h4>\n<p>If x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.<br>\nMore detail in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression\" target=\"_blank\" rel=\"noopener\">sklearn Mutual information</a><br>\nWhich is good for non-linear relation</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># x is feature df, y is target df</span></span><br><span class=\"line\"><span class=\"comment\"># For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X</span></span><br><span class=\"line\"><span class=\"comment\"># n_neighbor higher values reduce variance of the estimation</span></span><br><span class=\"line\">sklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=<span class=\"number\">3</span>, copy=<span class=\"literal\">True</span>, random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\">sklearn.feature_selection.mututal_info_classif(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output is estimated MI between each feature and target</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"variance-threshold\"><a class=\"markdownIt-Anchor\" href=\"#variance-threshold\"></a> Variance threshold</h4>\n<p>Which is only care about feature itself: if it not vary a lot, then it has poor predictive power.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure>"},{"title":"Generalized Linear Model","date":"2019-08-08T01:04:51.000Z","_content":"\nObjective:\n* General linear model (GLM) used to predict response variables with both continuous or discrete distribution and response variables can have linear or non-linear relationship with explanatory variables. \n\nModel structure: \n* GLM includes three parts. Random component, Systematic component, Link function.\n\nAssumption:  \n* $y_i$ is independent and typically assumed to follow an [exponential family](https://tedlsx.github.io/2019/08/06/exp=family/) distribution with $\\mu_i$. \n* GLM assumes transformed dependent variables (through link function) and independent variables has a linear relationship. \n* Error $\\epsilon_i$ should be independent but not normally distributed.\n\nParameter estimate: \n* $x_i$ as covariates and $\\beta$ for coefficients\n\nModel selection: \n* feature selection \n\nModel fit: \n* Least square \n<!--more-->\n\n## Generalized Linear Models (GLM)\n| Model | Random | Link | Systematic |\n| ----- | ------ | ---- | -----------|\n| linear regression | normal | identity | continuous |\n| ANOVA | normal | identity | ategorical |\n| logistic regression | binomial | logit | mixed |\n| loglinear | poisson | log | categorical |\n| poisson regression | poisson | log | mixed |\n| multinomial response | multinomial | generalized logit | mixed |\n\nIntroduction of 3 components of any GLM:\nrandom component: which is the probability distribution of response variable Y\n\nsystematic component: show the explanatory variables $(x_1,x_2,..,x_k)$ in the model\n\nlink function: $\\eta$ or $g(u)$ - refer to the link between random and systematic components. In another words, it shows how response variables change based on explanatory variables. It can be a non-linear function.\n\nThen we look some examples for GLM component:\n\n### simple linear regression\n\n$y_I = \\beta_0 + \\beta x_i + \\epsilon_i$\n\n* Random component: Y is dependent variable and has normal distribution, $\\epsilon_i \\sim N(0,\\sigma^2)$\n* Systematic component:  independent variable X can be continuous or discrete and is linear in $\\beta x_i$ \n* Link function: identity link , $\\eta=g(E(y_i))=E(y_i)$, this is the simple link function which model the mean directly.\n\n### Logistic regression\nFor binary response variable Y\n\n$logit(\\pi)=log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta x_i$\nwhich is the log odds of probability of \"success\" as a function of explanatory variables.\n\n* Random component: The distribution of Y is assumed as $Binomial(n,\\pi)$, $\\pi$ is probability of \"success\"\n* Systematic component: X is independent variables and can be continuous, discrete or both\n* link function: Log link, $\\eta = logit(\\pi)=log(\\frac{\\pi}{1-\\pi})$ which models the log odds of the mean($\\mu$).\n\n### Log-linear model\n$log(\\mu_{ij}) = \\lambda + \\lambda_i^A + \\lambda_j^B + \\lambda_{ij}^{AB}$\n\n* Random component: The distribution of counts, follows Poisson distribution\n* Systematic component: Independent variable X is discrete and is linear in parameters $\\lambda + \\lambda_i^{x_i}+...+ \\lambda_n^{x_n}$\n* Link function: Log link, $\\eta = log(\\mu)$ which model the log of mean.\n\n## Advantage of GLM over OLS(ordinary least square)\n* There is no need to transform the respose $Y$ to have normal distribution.\n* Variance no need to be constant","source":"_posts/glm.md","raw":"---\ntitle: Generalized Linear Model\ndate: 2019-08-08 09:04:51\ntags:\n---\n\nObjective:\n* General linear model (GLM) used to predict response variables with both continuous or discrete distribution and response variables can have linear or non-linear relationship with explanatory variables. \n\nModel structure: \n* GLM includes three parts. Random component, Systematic component, Link function.\n\nAssumption:  \n* $y_i$ is independent and typically assumed to follow an [exponential family](https://tedlsx.github.io/2019/08/06/exp=family/) distribution with $\\mu_i$. \n* GLM assumes transformed dependent variables (through link function) and independent variables has a linear relationship. \n* Error $\\epsilon_i$ should be independent but not normally distributed.\n\nParameter estimate: \n* $x_i$ as covariates and $\\beta$ for coefficients\n\nModel selection: \n* feature selection \n\nModel fit: \n* Least square \n<!--more-->\n\n## Generalized Linear Models (GLM)\n| Model | Random | Link | Systematic |\n| ----- | ------ | ---- | -----------|\n| linear regression | normal | identity | continuous |\n| ANOVA | normal | identity | ategorical |\n| logistic regression | binomial | logit | mixed |\n| loglinear | poisson | log | categorical |\n| poisson regression | poisson | log | mixed |\n| multinomial response | multinomial | generalized logit | mixed |\n\nIntroduction of 3 components of any GLM:\nrandom component: which is the probability distribution of response variable Y\n\nsystematic component: show the explanatory variables $(x_1,x_2,..,x_k)$ in the model\n\nlink function: $\\eta$ or $g(u)$ - refer to the link between random and systematic components. In another words, it shows how response variables change based on explanatory variables. It can be a non-linear function.\n\nThen we look some examples for GLM component:\n\n### simple linear regression\n\n$y_I = \\beta_0 + \\beta x_i + \\epsilon_i$\n\n* Random component: Y is dependent variable and has normal distribution, $\\epsilon_i \\sim N(0,\\sigma^2)$\n* Systematic component:  independent variable X can be continuous or discrete and is linear in $\\beta x_i$ \n* Link function: identity link , $\\eta=g(E(y_i))=E(y_i)$, this is the simple link function which model the mean directly.\n\n### Logistic regression\nFor binary response variable Y\n\n$logit(\\pi)=log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta x_i$\nwhich is the log odds of probability of \"success\" as a function of explanatory variables.\n\n* Random component: The distribution of Y is assumed as $Binomial(n,\\pi)$, $\\pi$ is probability of \"success\"\n* Systematic component: X is independent variables and can be continuous, discrete or both\n* link function: Log link, $\\eta = logit(\\pi)=log(\\frac{\\pi}{1-\\pi})$ which models the log odds of the mean($\\mu$).\n\n### Log-linear model\n$log(\\mu_{ij}) = \\lambda + \\lambda_i^A + \\lambda_j^B + \\lambda_{ij}^{AB}$\n\n* Random component: The distribution of counts, follows Poisson distribution\n* Systematic component: Independent variable X is discrete and is linear in parameters $\\lambda + \\lambda_i^{x_i}+...+ \\lambda_n^{x_n}$\n* Link function: Log link, $\\eta = log(\\mu)$ which model the log of mean.\n\n## Advantage of GLM over OLS(ordinary least square)\n* There is no need to transform the respose $Y$ to have normal distribution.\n* Variance no need to be constant","slug":"glm","published":1,"updated":"2019-08-08T12:40:32.410Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3ososs0004p1dlbqfklszi","content":"<p>Objective:</p>\n<ul>\n<li>General linear model (GLM) used to predict response variables with both continuous or discrete distribution and response variables can have linear or non-linear relationship with explanatory variables.</li>\n</ul>\n<p>Model structure:</p>\n<ul>\n<li>GLM includes three parts. Random component, Systematic component, Link function.</li>\n</ul>\n<p>Assumption:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is independent and typically assumed to follow an <a href=\"https://tedlsx.github.io/2019/08/06/exp=family/\">exponential family</a> distribution with <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>μ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>.</li>\n<li>GLM assumes transformed dependent variables (through link function) and independent variables has a linear relationship.</li>\n<li>Error <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> should be independent but not normally distributed.</li>\n</ul>\n<p>Parameter estimate:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> as covariates and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span> for coefficients</li>\n</ul>\n<p>Model selection:</p>\n<ul>\n<li>feature selection</li>\n</ul>\n<p>Model fit:</p>\n<ul>\n<li>Least square</li>\n</ul>\n<a id=\"more\"></a>\n<h2 id=\"generalized-linear-models-glm\"><a class=\"markdownIt-Anchor\" href=\"#generalized-linear-models-glm\"></a> Generalized Linear Models (GLM)</h2>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Random</th>\n<th>Link</th>\n<th>Systematic</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>linear regression</td>\n<td>normal</td>\n<td>identity</td>\n<td>continuous</td>\n</tr>\n<tr>\n<td>ANOVA</td>\n<td>normal</td>\n<td>identity</td>\n<td>ategorical</td>\n</tr>\n<tr>\n<td>logistic regression</td>\n<td>binomial</td>\n<td>logit</td>\n<td>mixed</td>\n</tr>\n<tr>\n<td>loglinear</td>\n<td>poisson</td>\n<td>log</td>\n<td>categorical</td>\n</tr>\n<tr>\n<td>poisson regression</td>\n<td>poisson</td>\n<td>log</td>\n<td>mixed</td>\n</tr>\n<tr>\n<td>multinomial response</td>\n<td>multinomial</td>\n<td>generalized logit</td>\n<td>mixed</td>\n</tr>\n</tbody>\n</table>\n<p>Introduction of 3 components of any GLM:<br>\nrandom component: which is the probability distribution of response variable Y</p>\n<p>systematic component: show the explanatory variables <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_1,x_2,..,x_k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> in the model</p>\n<p>link function: <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span></span></span></span> or <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">g(u)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">u</span><span class=\"mclose\">)</span></span></span></span> - refer to the link between random and systematic components. In another words, it shows how response variables change based on explanatory variables. It can be a non-linear function.</p>\n<p>Then we look some examples for GLM component:</p>\n<h3 id=\"simple-linear-regression\"><a class=\"markdownIt-Anchor\" href=\"#simple-linear-regression\"></a> simple linear regression</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>y</mi><mi>I</mi></msub><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_I = \\beta_0 + \\beta x_i + \\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07847em;\">I</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></p>\n<ul>\n<li>Random component: Y is dependent variable and has normal distribution, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i \\sim N(0,\\sigma^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></li>\n<li>Systematic component:  independent variable X can be continuous or discrete and is linear in <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\beta x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li>Link function: identity link , <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\eta=g(E(y_i))=E(y_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, this is the simple link function which model the mean directly.</li>\n</ul>\n<h3 id=\"logistic-regression\"><a class=\"markdownIt-Anchor\" href=\"#logistic-regression\"></a> Logistic regression</h3>\n<p>For binary response variable Y</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy=\"false\">(</mo><mi>π</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mfrac><mi>π</mi><mrow><mn>1</mn><mo>−</mo><mi>π</mi></mrow></mfrac><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">logit(\\pi)=log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.153331em;vertical-align:-0.403331em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.695392em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><br>\nwhich is the log odds of probability of “success” as a function of explanatory variables.</p>\n<ul>\n<li>Random component: The distribution of Y is assumed as <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>B</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>m</mi><mi>i</mi><mi>a</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo separator=\"true\">,</mo><mi>π</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Binomial(n,\\pi)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span></span></span></span> is probability of “success”</li>\n<li>Systematic component: X is independent variables and can be continuous, discrete or both</li>\n<li>link function: Log link, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy=\"false\">(</mo><mi>π</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mfrac><mi>π</mi><mrow><mn>1</mn><mo>−</mo><mi>π</mi></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\eta = logit(\\pi)=log(\\frac{\\pi}{1-\\pi})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.153331em;vertical-align:-0.403331em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.695392em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span> which models the log odds of the mean(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>).</li>\n</ul>\n<h3 id=\"log-linear-model\"><a class=\"markdownIt-Anchor\" href=\"#log-linear-model\"></a> Log-linear model</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>λ</mi><mo>+</mo><msubsup><mi>λ</mi><mi>i</mi><mi>A</mi></msubsup><mo>+</mo><msubsup><mi>λ</mi><mi>j</mi><mi>B</mi></msubsup><mo>+</mo><msubsup><mi>λ</mi><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mi>A</mi><mi>B</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">log(\\mu_{ij}) = \\lambda + \\lambda_i^A + \\lambda_j^B + \\lambda_{ij}^{AB}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">λ</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0999949999999998em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">A</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.236103em;vertical-align:-0.394772em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.394772em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.236103em;vertical-align:-0.394772em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">A</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.394772em;\"><span></span></span></span></span></span></span></span></span></span></p>\n<ul>\n<li>Random component: The distribution of counts, follows Poisson distribution</li>\n<li>Systematic component: Independent variable X is discrete and is linear in parameters <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>λ</mi><mo>+</mo><msubsup><mi>λ</mi><mi>i</mi><msub><mi>x</mi><mi>i</mi></msub></msubsup><mo>+</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo>+</mo><msubsup><mi>λ</mi><mi>n</mi><msub><mi>x</mi><mi>n</mi></msub></msubsup></mrow><annotation encoding=\"application/x-tex\">\\lambda + \\lambda_i^{x_i}+...+ \\lambda_n^{x_n}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">λ</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.023156em;vertical-align:-0.276864em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.746292em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.1449000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.276864em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9414399999999999em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.16454285714285719em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li>Link function: Log link, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>μ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\eta = log(\\mu)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">μ</span><span class=\"mclose\">)</span></span></span></span> which model the log of mean.</li>\n</ul>\n<h2 id=\"advantage-of-glm-over-olsordinary-least-square\"><a class=\"markdownIt-Anchor\" href=\"#advantage-of-glm-over-olsordinary-least-square\"></a> Advantage of GLM over OLS(ordinary least square)</h2>\n<ul>\n<li>There is no need to transform the respose <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> to have normal distribution.</li>\n<li>Variance no need to be constant</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Objective:</p>\n<ul>\n<li>General linear model (GLM) used to predict response variables with both continuous or discrete distribution and response variables can have linear or non-linear relationship with explanatory variables.</li>\n</ul>\n<p>Model structure:</p>\n<ul>\n<li>GLM includes three parts. Random component, Systematic component, Link function.</li>\n</ul>\n<p>Assumption:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is independent and typically assumed to follow an <a href=\"https://tedlsx.github.io/2019/08/06/exp=family/\">exponential family</a> distribution with <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>μ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>.</li>\n<li>GLM assumes transformed dependent variables (through link function) and independent variables has a linear relationship.</li>\n<li>Error <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> should be independent but not normally distributed.</li>\n</ul>\n<p>Parameter estimate:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> as covariates and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span> for coefficients</li>\n</ul>\n<p>Model selection:</p>\n<ul>\n<li>feature selection</li>\n</ul>\n<p>Model fit:</p>\n<ul>\n<li>Least square</li>\n</ul>","more":"<h2 id=\"generalized-linear-models-glm\"><a class=\"markdownIt-Anchor\" href=\"#generalized-linear-models-glm\"></a> Generalized Linear Models (GLM)</h2>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Random</th>\n<th>Link</th>\n<th>Systematic</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>linear regression</td>\n<td>normal</td>\n<td>identity</td>\n<td>continuous</td>\n</tr>\n<tr>\n<td>ANOVA</td>\n<td>normal</td>\n<td>identity</td>\n<td>ategorical</td>\n</tr>\n<tr>\n<td>logistic regression</td>\n<td>binomial</td>\n<td>logit</td>\n<td>mixed</td>\n</tr>\n<tr>\n<td>loglinear</td>\n<td>poisson</td>\n<td>log</td>\n<td>categorical</td>\n</tr>\n<tr>\n<td>poisson regression</td>\n<td>poisson</td>\n<td>log</td>\n<td>mixed</td>\n</tr>\n<tr>\n<td>multinomial response</td>\n<td>multinomial</td>\n<td>generalized logit</td>\n<td>mixed</td>\n</tr>\n</tbody>\n</table>\n<p>Introduction of 3 components of any GLM:<br>\nrandom component: which is the probability distribution of response variable Y</p>\n<p>systematic component: show the explanatory variables <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_1,x_2,..,x_k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> in the model</p>\n<p>link function: <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span></span></span></span> or <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">g(u)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">u</span><span class=\"mclose\">)</span></span></span></span> - refer to the link between random and systematic components. In another words, it shows how response variables change based on explanatory variables. It can be a non-linear function.</p>\n<p>Then we look some examples for GLM component:</p>\n<h3 id=\"simple-linear-regression\"><a class=\"markdownIt-Anchor\" href=\"#simple-linear-regression\"></a> simple linear regression</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>y</mi><mi>I</mi></msub><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_I = \\beta_0 + \\beta x_i + \\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07847em;\">I</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></p>\n<ul>\n<li>Random component: Y is dependent variable and has normal distribution, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i \\sim N(0,\\sigma^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></li>\n<li>Systematic component:  independent variable X can be continuous or discrete and is linear in <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\beta x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li>Link function: identity link , <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\eta=g(E(y_i))=E(y_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, this is the simple link function which model the mean directly.</li>\n</ul>\n<h3 id=\"logistic-regression\"><a class=\"markdownIt-Anchor\" href=\"#logistic-regression\"></a> Logistic regression</h3>\n<p>For binary response variable Y</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy=\"false\">(</mo><mi>π</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mfrac><mi>π</mi><mrow><mn>1</mn><mo>−</mo><mi>π</mi></mrow></mfrac><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">logit(\\pi)=log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.153331em;vertical-align:-0.403331em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.695392em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><br>\nwhich is the log odds of probability of “success” as a function of explanatory variables.</p>\n<ul>\n<li>Random component: The distribution of Y is assumed as <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>B</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>m</mi><mi>i</mi><mi>a</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo separator=\"true\">,</mo><mi>π</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Binomial(n,\\pi)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span></span></span></span> is probability of “success”</li>\n<li>Systematic component: X is independent variables and can be continuous, discrete or both</li>\n<li>link function: Log link, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy=\"false\">(</mo><mi>π</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mfrac><mi>π</mi><mrow><mn>1</mn><mo>−</mo><mi>π</mi></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\eta = logit(\\pi)=log(\\frac{\\pi}{1-\\pi})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.153331em;vertical-align:-0.403331em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.695392em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span> which models the log odds of the mean(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>).</li>\n</ul>\n<h3 id=\"log-linear-model\"><a class=\"markdownIt-Anchor\" href=\"#log-linear-model\"></a> Log-linear model</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>λ</mi><mo>+</mo><msubsup><mi>λ</mi><mi>i</mi><mi>A</mi></msubsup><mo>+</mo><msubsup><mi>λ</mi><mi>j</mi><mi>B</mi></msubsup><mo>+</mo><msubsup><mi>λ</mi><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mi>A</mi><mi>B</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">log(\\mu_{ij}) = \\lambda + \\lambda_i^A + \\lambda_j^B + \\lambda_{ij}^{AB}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">λ</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0999949999999998em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">A</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.236103em;vertical-align:-0.394772em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.394772em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.236103em;vertical-align:-0.394772em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">A</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.394772em;\"><span></span></span></span></span></span></span></span></span></span></p>\n<ul>\n<li>Random component: The distribution of counts, follows Poisson distribution</li>\n<li>Systematic component: Independent variable X is discrete and is linear in parameters <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>λ</mi><mo>+</mo><msubsup><mi>λ</mi><mi>i</mi><msub><mi>x</mi><mi>i</mi></msub></msubsup><mo>+</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo>+</mo><msubsup><mi>λ</mi><mi>n</mi><msub><mi>x</mi><mi>n</mi></msub></msubsup></mrow><annotation encoding=\"application/x-tex\">\\lambda + \\lambda_i^{x_i}+...+ \\lambda_n^{x_n}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">λ</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.023156em;vertical-align:-0.276864em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.746292em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.1449000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.276864em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9414399999999999em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.16454285714285719em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li>Link function: Log link, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>μ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\eta = log(\\mu)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">μ</span><span class=\"mclose\">)</span></span></span></span> which model the log of mean.</li>\n</ul>\n<h2 id=\"advantage-of-glm-over-olsordinary-least-square\"><a class=\"markdownIt-Anchor\" href=\"#advantage-of-glm-over-olsordinary-least-square\"></a> Advantage of GLM over OLS(ordinary least square)</h2>\n<ul>\n<li>There is no need to transform the respose <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> to have normal distribution.</li>\n<li>Variance no need to be constant</li>\n</ul>"},{"title":"Linear Regression Model","date":"2019-08-05T07:04:51.000Z","_content":"Objective: \n* Trying to predict the continuous variable Y which is a linear function of several continus variables x.\n\nModel structure: \n* $Y_i=\\beta_0 + \\beta_1 x_i + \\epsilon_i$\n\nAssumption: \n* Y follows normal distribution, error $\\epsilon_i$ is indepdent and has $\\epsilon_i \\sim N(0,\\sigma^2)$. Data X is fixed \n\nParameter estimate: \n* $\\beta_0$ as intercept and $\\beta_1$ as slope\n\nModel selection: \n* feature selection \n\nModel fit: \n* $R^2$\n* residual analysis\n* F-statistic\n<!--more-->\n\n## Multiple Linear Regression Model\nMultiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.\n\nWhere the formula for Multiple linear regression model is:\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\"/>\n</p> \nfor i = 1,...,n is the number of observations or data. $p$ is the number of dependent variables. \n\nHence the matrix notation for multiple linear regression is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\"/>\n</p> \n\nWhich can also write in simple statement:\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\"/>\n</p> \n\nUsing leat square, we need to minimise this function \n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\"/>\n</p>\n\nTo find the 'best'  $\\beta$. One way is to find the relation between $y-\\hat{y}$ and $X$:\n\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\"/>\n</p> \n\n![A test image](linear-model/orth.png)\n\nIn this figure, the residuals $y-\\hat{y}$ are [orthogonal](http://mathworld.wolfram.com/Orthogonal.html) to the columns of $X$:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\"/>\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\"/>\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\"/>\n</p> \n\nThen we can define \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^Ty\"/>\n</p> \n\nAlso we can use **Least Squares** as our loss(error) function which can minimize the Eucledian distance between the predicted $\\hat{y}$ and actual $y$:\n\n$Loss function =L = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\beta^Tx_i)^2=\\frac{1}{2}||y-\\beta x||^2 = \\frac{1}{2}(y-x\\beta)^T(y-x\\beta)$\n\nFinding the minium of the loss function, we can use differentiate for $\\beta$:\n\n$\\frac{dL}{d\\beta}=-X^Ty+X^TX\\beta=0$\n\nWe still got the result:\n\n$\\hat{\\beta}=(X^TX)^{-1}X^Ty$\n\nThis of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.\n\nFor fitted $\\hat{y}$, we can plug in the $\\hat{\\beta}$\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\"/>\n</p> \n\nThe matrix $H$ (Hat-matrix) is a $n*n$ matrix, it maps the observed values $y$ onto the fitted value $\\hat{y}$\n\nAnd residuals can be written as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\"/>\n</p> \n\nHere is a comparison between my own code build with [numpy](https://numpy.org) performance and the package in [sckit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.htmln)\n```python\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n# y = 1 * x_0 + 2 * x_1 + 3\ny = np.dot(X, np.array([1, 2])) + 3\n\n## add a column with value 1 at the left side of x \nx_with_constant = np.insert(x,0,1, axis=1)\nx_t = np.transpose(x_with_constant)\n\nbeta = np.linalg.inv(x_t.dot(x_with_constant)).dot(x_t).dot(y)\n## beta is array([3., 1., 2.])\ny_pred =  np.array([[1,3,5]]).dot(beta)\n\ny_pred\n## array([16.])\n```\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nx = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n# y = 1 * x_0 + 2 * x_1 + 3\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression().fit(X, y)\nreg.score(X, y)\n###  1.0\neg.coef_\n## array([1., 2.])\nreg.intercept_ \n### 3.0000...\nreg.predict(np.array([[3, 5]]))\n## array([16.])\n```\n\nComparing to the function in sckit-learn, both of them can do the correct prediction. And other performance can be consider later...\n\n## Reference \n\n[1] http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf","source":"_posts/linear-model.md","raw":"---\ntitle:  Linear Regression Model\ndate: 2019-08-05 15:04:51\ntags:\n---\nObjective: \n* Trying to predict the continuous variable Y which is a linear function of several continus variables x.\n\nModel structure: \n* $Y_i=\\beta_0 + \\beta_1 x_i + \\epsilon_i$\n\nAssumption: \n* Y follows normal distribution, error $\\epsilon_i$ is indepdent and has $\\epsilon_i \\sim N(0,\\sigma^2)$. Data X is fixed \n\nParameter estimate: \n* $\\beta_0$ as intercept and $\\beta_1$ as slope\n\nModel selection: \n* feature selection \n\nModel fit: \n* $R^2$\n* residual analysis\n* F-statistic\n<!--more-->\n\n## Multiple Linear Regression Model\nMultiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.\n\nWhere the formula for Multiple linear regression model is:\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\"/>\n</p> \nfor i = 1,...,n is the number of observations or data. $p$ is the number of dependent variables. \n\nHence the matrix notation for multiple linear regression is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\"/>\n</p> \n\nWhich can also write in simple statement:\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\"/>\n</p> \n\nUsing leat square, we need to minimise this function \n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\"/>\n</p>\n\nTo find the 'best'  $\\beta$. One way is to find the relation between $y-\\hat{y}$ and $X$:\n\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\"/>\n</p> \n\n![A test image](linear-model/orth.png)\n\nIn this figure, the residuals $y-\\hat{y}$ are [orthogonal](http://mathworld.wolfram.com/Orthogonal.html) to the columns of $X$:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\"/>\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\"/>\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\"/>\n</p> \n\nThen we can define \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^Ty\"/>\n</p> \n\nAlso we can use **Least Squares** as our loss(error) function which can minimize the Eucledian distance between the predicted $\\hat{y}$ and actual $y$:\n\n$Loss function =L = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\beta^Tx_i)^2=\\frac{1}{2}||y-\\beta x||^2 = \\frac{1}{2}(y-x\\beta)^T(y-x\\beta)$\n\nFinding the minium of the loss function, we can use differentiate for $\\beta$:\n\n$\\frac{dL}{d\\beta}=-X^Ty+X^TX\\beta=0$\n\nWe still got the result:\n\n$\\hat{\\beta}=(X^TX)^{-1}X^Ty$\n\nThis of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.\n\nFor fitted $\\hat{y}$, we can plug in the $\\hat{\\beta}$\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\"/>\n</p> \n\nThe matrix $H$ (Hat-matrix) is a $n*n$ matrix, it maps the observed values $y$ onto the fitted value $\\hat{y}$\n\nAnd residuals can be written as \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\"/>\n</p> \n\nHere is a comparison between my own code build with [numpy](https://numpy.org) performance and the package in [sckit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.htmln)\n```python\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n# y = 1 * x_0 + 2 * x_1 + 3\ny = np.dot(X, np.array([1, 2])) + 3\n\n## add a column with value 1 at the left side of x \nx_with_constant = np.insert(x,0,1, axis=1)\nx_t = np.transpose(x_with_constant)\n\nbeta = np.linalg.inv(x_t.dot(x_with_constant)).dot(x_t).dot(y)\n## beta is array([3., 1., 2.])\ny_pred =  np.array([[1,3,5]]).dot(beta)\n\ny_pred\n## array([16.])\n```\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nx = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n# y = 1 * x_0 + 2 * x_1 + 3\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression().fit(X, y)\nreg.score(X, y)\n###  1.0\neg.coef_\n## array([1., 2.])\nreg.intercept_ \n### 3.0000...\nreg.predict(np.array([[3, 5]]))\n## array([16.])\n```\n\nComparing to the function in sckit-learn, both of them can do the correct prediction. And other performance can be consider later...\n\n## Reference \n\n[1] http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf","slug":"linear-model","published":1,"updated":"2019-08-08T09:55:07.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3osost0005p1dl2v3bgoeg","content":"<p>Objective:</p>\n<ul>\n<li>Trying to predict the continuous variable Y which is a linear function of several continus variables x.</li>\n</ul>\n<p>Model structure:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Y_i=\\beta_0 + \\beta_1 x_i + \\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<p>Assumption:</p>\n<ul>\n<li>Y follows normal distribution, error <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is indepdent and has <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i \\sim N(0,\\sigma^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>. Data X is fixed</li>\n</ul>\n<p>Parameter estimate:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\beta_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> as intercept and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\beta_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> as slope</li>\n</ul>\n<p>Model selection:</p>\n<ul>\n<li>feature selection</li>\n</ul>\n<p>Model fit:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">R^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></li>\n<li>residual analysis</li>\n<li>F-statistic</li>\n</ul>\n<a id=\"more\"></a>\n<h2 id=\"multiple-linear-regression-model\"><a class=\"markdownIt-Anchor\" href=\"#multiple-linear-regression-model\"></a> Multiple Linear Regression Model</h2>\n<p>Multiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.</p>\n<p>Where the formula for Multiple linear regression model is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\">\n</p> \nfor i = 1,...,n is the number of observations or data. $p$ is the number of dependent variables. \n<p>Hence the matrix notation for multiple linear regression is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\">\n</p> \n<p>Which can also write in simple statement:</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\">\n</p> \n<p>Using leat square, we need to minimise this function</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\">\n</p>\n<p>To find the ‘best’  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>. One way is to find the relation between <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>:</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\">\n</p> \n<p><img src=\"/2019/08/05/linear-model/orth.png\" alt=\"A test image\"></p>\n<p>In this figure, the residuals <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> are <a href=\"http://mathworld.wolfram.com/Orthogonal.html\" target=\"_blank\" rel=\"noopener\">orthogonal</a> to the columns of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\">\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\">\n</p> \n<p>Then we can define</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^Ty\">\n</p> \n<p>Also we can use <strong>Least Squares</strong> as our loss(error) function which can minimize the Eucledian distance between the predicted <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> and actual <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msup><mi>β</mi><mi>T</mi></msup><msub><mi>x</mi><mi>i</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo>−</mo><mi>β</mi><mi>x</mi><mi mathvariant=\"normal\">∣</mi><msup><mi mathvariant=\"normal\">∣</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy=\"false\">(</mo><mi>y</mi><mo>−</mo><mi>x</mi><mi>β</mi><msup><mo stretchy=\"false\">)</mo><mi>T</mi></msup><mo stretchy=\"false\">(</mo><mi>y</mi><mo>−</mo><mi>x</mi><mi>β</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Loss function =L = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\beta^Tx_i)^2=\\frac{1}{2}||y-\\beta x||^2 = \\frac{1}{2}(y-x\\beta)^T(y-x\\beta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mclose\">)</span></span></span></span></p>\n<p>Finding the minium of the loss function, we can use differentiate for <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><mi>β</mi></mrow></mfrac><mo>=</mo><mo>−</mo><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi><mo>+</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><mi>β</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\frac{dL}{d\\beta}=-X^Ty+X^TX\\beta=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.3612159999999998em;vertical-align:-0.481108em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05278em;\">β</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.481108em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.035771em;vertical-align:-0.19444em;\"></span><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.035771em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span></p>\n<p>We still got the result:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover><mo>=</mo><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">\\hat{\\beta}=(X^TX)^{-1}X^Ty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span></p>\n<p>This of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.</p>\n<p>For fitted <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>, we can plug in the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{\\beta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\">\n</p> \n<p>The matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> (Hat-matrix) is a <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi><mo>∗</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n*n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> matrix, it maps the observed values <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> onto the fitted value <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p>And residuals can be written as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\">\n</p> \n<p>Here is a comparison between my own code build with <a href=\"https://numpy.org\" target=\"_blank\" rel=\"noopener\">numpy</a> performance and the package in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.htmln\" target=\"_blank\" rel=\"noopener\">sckit-learn</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.array([[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">3</span>]])</span><br><span class=\"line\"><span class=\"comment\"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class=\"line\">y = np.dot(X, np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])) + <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## add a column with value 1 at the left side of x </span></span><br><span class=\"line\">x_with_constant = np.insert(x,<span class=\"number\">0</span>,<span class=\"number\">1</span>, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">x_t = np.transpose(x_with_constant)</span><br><span class=\"line\"></span><br><span class=\"line\">beta = np.linalg.inv(x_t.dot(x_with_constant)).dot(x_t).dot(y)</span><br><span class=\"line\"><span class=\"comment\">## beta is array([3., 1., 2.])</span></span><br><span class=\"line\">y_pred =  np.array([[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>]]).dot(beta)</span><br><span class=\"line\"></span><br><span class=\"line\">y_pred</span><br><span class=\"line\"><span class=\"comment\">## array([16.])</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\">x = np.array([[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">3</span>]])</span><br><span class=\"line\"><span class=\"comment\"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class=\"line\">y = np.dot(X, np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])) + <span class=\"number\">3</span></span><br><span class=\"line\">reg = LinearRegression().fit(X, y)</span><br><span class=\"line\">reg.score(X, y)</span><br><span class=\"line\"><span class=\"comment\">###  1.0</span></span><br><span class=\"line\">eg.coef_</span><br><span class=\"line\"><span class=\"comment\">## array([1., 2.])</span></span><br><span class=\"line\">reg.intercept_ </span><br><span class=\"line\"><span class=\"comment\">### 3.0000...</span></span><br><span class=\"line\">reg.predict(np.array([[<span class=\"number\">3</span>, <span class=\"number\">5</span>]]))</span><br><span class=\"line\"><span class=\"comment\">## array([16.])</span></span><br></pre></td></tr></table></figure>\n<p>Comparing to the function in sckit-learn, both of them can do the correct prediction. And other performance can be consider later…</p>\n<h2 id=\"reference\"><a class=\"markdownIt-Anchor\" href=\"#reference\"></a> Reference</h2>\n<p>[1] <a href=\"http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf\" target=\"_blank\" rel=\"noopener\">http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement 5 - multiple regression.pdf</a></p>\n","site":{"data":{}},"excerpt":"<p>Objective:</p>\n<ul>\n<li>Trying to predict the continuous variable Y which is a linear function of several continus variables x.</li>\n</ul>\n<p>Model structure:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Y_i=\\beta_0 + \\beta_1 x_i + \\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<p>Assumption:</p>\n<ul>\n<li>Y follows normal distribution, error <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is indepdent and has <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon_i \\sim N(0,\\sigma^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">ϵ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>. Data X is fixed</li>\n</ul>\n<p>Parameter estimate:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\beta_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> as intercept and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\beta_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> as slope</li>\n</ul>\n<p>Model selection:</p>\n<ul>\n<li>feature selection</li>\n</ul>\n<p>Model fit:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">R^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></li>\n<li>residual analysis</li>\n<li>F-statistic</li>\n</ul>","more":"<h2 id=\"multiple-linear-regression-model\"><a class=\"markdownIt-Anchor\" href=\"#multiple-linear-regression-model\"></a> Multiple Linear Regression Model</h2>\n<p>Multiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.</p>\n<p>Where the formula for Multiple linear regression model is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_nx_{ip}+\\epsilon_n\">\n</p> \nfor i = 1,...,n is the number of observations or data. $p$ is the number of dependent variables. \n<p>Hence the matrix notation for multiple linear regression is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots \\\\\n    y_n\n\\end{bmatrix}\n=\\begin{bmatrix}\n    1 & x_{11} & x_{12} & \\dots  & x_{1n} \\\\\n    1 & x_{21} & x_{22} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{d1} & x_{d2} & \\dots  & x_{dn}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\vdots \\\\\n    \\beta_n\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    \\epsilon_1 \\\\\n    \\epsilon_2 \\\\\n    \\vdots \\\\\n    \\epsilon_n\n\\end{bmatrix}\n\">\n</p> \n<p>Which can also write in simple statement:</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; Y=X \\beta+\\epsilon\">\n</p> \n<p>Using leat square, we need to minimise this function</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\sum_{i=1}^{n}\\epsilopn_i^2=\\epsilon^T\\epsilon=(y-X\\beta)^T(y-X\\beta)\">\n</p>\n<p>To find the ‘best’  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>. One way is to find the relation between <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>:</p>\n <p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{Y}=X \\hat{\\beta}\">\n</p> \n<p><img src=\"/2019/08/05/linear-model/orth.png\" alt=\"A test image\"></p>\n<p>In this figure, the residuals <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> are <a href=\"http://mathworld.wolfram.com/Orthogonal.html\" target=\"_blank\" rel=\"noopener\">orthogonal</a> to the columns of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; X^T(y-X\\hat{\\beta})=0\">\n</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^Ty-X^TX\\hat{\\beta}=0\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\Leftrightarrow X^TX\\hat{\\beta}=X^Ty\">\n</p> \n<p>Then we can define</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{\\beta}=(X^TX)^{-1}X^Ty\">\n</p> \n<p>Also we can use <strong>Least Squares</strong> as our loss(error) function which can minimize the Eucledian distance between the predicted <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> and actual <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msup><mi>β</mi><mi>T</mi></msup><msub><mi>x</mi><mi>i</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo>−</mo><mi>β</mi><mi>x</mi><mi mathvariant=\"normal\">∣</mi><msup><mi mathvariant=\"normal\">∣</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy=\"false\">(</mo><mi>y</mi><mo>−</mo><mi>x</mi><mi>β</mi><msup><mo stretchy=\"false\">)</mo><mi>T</mi></msup><mo stretchy=\"false\">(</mo><mi>y</mi><mo>−</mo><mi>x</mi><mi>β</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Loss function =L = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\beta^Tx_i)^2=\\frac{1}{2}||y-\\beta x||^2 = \\frac{1}{2}(y-x\\beta)^T(y-x\\beta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mclose\">)</span></span></span></span></p>\n<p>Finding the minium of the loss function, we can use differentiate for <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><mi>β</mi></mrow></mfrac><mo>=</mo><mo>−</mo><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi><mo>+</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><mi>β</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\frac{dL}{d\\beta}=-X^Ty+X^TX\\beta=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.3612159999999998em;vertical-align:-0.481108em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05278em;\">β</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.481108em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.035771em;vertical-align:-0.19444em;\"></span><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.035771em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span></p>\n<p>We still got the result:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover><mo>=</mo><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">\\hat{\\beta}=(X^TX)^{-1}X^Ty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span></p>\n<p>This of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.</p>\n<p>For fitted <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>, we can plug in the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{\\beta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\hat{y}=X\\hat{\\beta}=X(X^TX)^{-1}X^Ty=Hy\">\n</p> \n<p>The matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> (Hat-matrix) is a <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi><mo>∗</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n*n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> matrix, it maps the observed values <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> onto the fitted value <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></p>\n<p>And residuals can be written as</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; \\epsilon=y-\\hat{y}=y-X\\hat{\\beta}=y-Hy=(I-H)y\">\n</p> \n<p>Here is a comparison between my own code build with <a href=\"https://numpy.org\" target=\"_blank\" rel=\"noopener\">numpy</a> performance and the package in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.htmln\" target=\"_blank\" rel=\"noopener\">sckit-learn</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.array([[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">3</span>]])</span><br><span class=\"line\"><span class=\"comment\"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class=\"line\">y = np.dot(X, np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])) + <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## add a column with value 1 at the left side of x </span></span><br><span class=\"line\">x_with_constant = np.insert(x,<span class=\"number\">0</span>,<span class=\"number\">1</span>, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">x_t = np.transpose(x_with_constant)</span><br><span class=\"line\"></span><br><span class=\"line\">beta = np.linalg.inv(x_t.dot(x_with_constant)).dot(x_t).dot(y)</span><br><span class=\"line\"><span class=\"comment\">## beta is array([3., 1., 2.])</span></span><br><span class=\"line\">y_pred =  np.array([[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>]]).dot(beta)</span><br><span class=\"line\"></span><br><span class=\"line\">y_pred</span><br><span class=\"line\"><span class=\"comment\">## array([16.])</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\">x = np.array([[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">3</span>]])</span><br><span class=\"line\"><span class=\"comment\"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class=\"line\">y = np.dot(X, np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])) + <span class=\"number\">3</span></span><br><span class=\"line\">reg = LinearRegression().fit(X, y)</span><br><span class=\"line\">reg.score(X, y)</span><br><span class=\"line\"><span class=\"comment\">###  1.0</span></span><br><span class=\"line\">eg.coef_</span><br><span class=\"line\"><span class=\"comment\">## array([1., 2.])</span></span><br><span class=\"line\">reg.intercept_ </span><br><span class=\"line\"><span class=\"comment\">### 3.0000...</span></span><br><span class=\"line\">reg.predict(np.array([[<span class=\"number\">3</span>, <span class=\"number\">5</span>]]))</span><br><span class=\"line\"><span class=\"comment\">## array([16.])</span></span><br></pre></td></tr></table></figure>\n<p>Comparing to the function in sckit-learn, both of them can do the correct prediction. And other performance can be consider later…</p>\n<h2 id=\"reference\"><a class=\"markdownIt-Anchor\" href=\"#reference\"></a> Reference</h2>\n<p>[1] <a href=\"http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf\" target=\"_blank\" rel=\"noopener\">http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement 5 - multiple regression.pdf</a></p>"},{"title":"Machine Learning","_content":"Introduction of several packages in machine learning.\n<!--more--> \n\nBefore we use many ML algorithm, we sometimes need to preprocess the data\n\nImport all package\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels as sm\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n#import model package\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n```\n\n## Preprocessing\nFunction to encode the data type, change all object type features into dummy variables \n```python \ndef encoder(df):\n    for column in df.columns:\n        if df[column].dtype == type(object):\n            le = preprocessing.LabelEncoder()\n            df[column] = le.fit_transform(df[column])\n    return df\n```\nFunction to normalise the data\n```python\ndef normalised(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(df.values)\n    df_new = pd.DataFrame(x_scaled, columns= df.columns)\n    return df_new\n```\n\nOr we can use the package in scikit learn of [data transformation](https://scikit-learn.org/stable/data_transforms.html). Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.\n```python\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_tranform(x_train)\nsc.tranform(x_test)\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit_tranform(x_train)\nmms.tranform(x_test)\n```\n\nUsing PCA as an example of reducing dimension\n```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\npca.fit_transdorm()\n```\n\n\n\nSpliting the data frame into train and test to varify the goodness of model\n```python\n# a fixed randome_state num will have same train and test set \ntrain_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[\"prod_id\"]], test_size=0.2, random_state=42)\n```\nLogistic Regression Classification\n```python\ndef logistic_regression_classifier(train_x, train_y):\n    model = LogisticRegression(penalty='l2', solver = \"lbfgs\", multi_class='auto')\n    model.fit(train_x, train_y)\n    return model\n\nlgr_model = logistic_regression_classifier(train_x, train_y)\npred_y = lgr_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nNaive Bayes Classification\n```python\ndef naive_bayes_classifier(x, y):\n    model = GaussianNB()\n    model.fit(x, y)\n    return model\n\nnb_model = naive_bayes_classifier(train_x, train_y)\npred_y = nb_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nKNN K-Nearest Neighbors Classification\n```python\ndef knn_classifier(x, y):\n    model = KNeighborsClassifier()\n    model.fit(train_x, train_y)\n    return model \n\nknn_model = knn_classifier(train_x, train_y)\npred_y = knn_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nRandom Forest Classifier\n```python\ndef random_forest_classifier(train_x, train_y):\n    model = RandomForestClassifier(max_depth=2, random_state=0, )\n    model.fit(train_x, train_y)\n    return model\n\nrf_model = random_forest_classifier(train_x, train_y)\nrf_model.feature_importances_  # show feature importance for each feature\nmodel = SelectFromModel(rf_model, prefit=True) # select no zero coefficient features\nx_new = model.transform(x)\nx_new.shape()\n\ntrain_x_rf = train_x.iloc[:, my_list] # can mutually define my_list = [] to select important feature\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nDecision Tree Classifier\n```python\ndef decision_tree_classifier(train_x, train_y):\n    model = tree.DecisionTreeClassifier()\n    model.fit(train_x, train_y)\n    return model\n\ndt_model = decision_tree_classifier(train_x, train_y)\npred_y = dt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nGBDT-Gradient Boosting Decision Tree Classification\n```python\ndef gradient_boosting_classifier(train_x, train_y):\n    model = GradientBoostingClassifier(n_estimators=200)\n    model.fit(train_x, train_y)\n    return model\n\ngbdt_model = gradient_boosting_classifier(train_x, train_y)\npred_y = gbdt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nSVM-Support Vaector Machine Classification\n```python\ndef svm_classifier(train_x, train_y):\n    model = SVC(kernel='rbf', probability=True, gamma = \"auto\")\n    model.fit(train_x, train_y)\n    return model\n\nsvm_model = svm_classifier(train_x, train_y)\npred_y = svm_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```","source":"_posts/machine_learning.md","raw":"---\ntitle: Machine Learning \n---\nIntroduction of several packages in machine learning.\n<!--more--> \n\nBefore we use many ML algorithm, we sometimes need to preprocess the data\n\nImport all package\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels as sm\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n#import model package\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n```\n\n## Preprocessing\nFunction to encode the data type, change all object type features into dummy variables \n```python \ndef encoder(df):\n    for column in df.columns:\n        if df[column].dtype == type(object):\n            le = preprocessing.LabelEncoder()\n            df[column] = le.fit_transform(df[column])\n    return df\n```\nFunction to normalise the data\n```python\ndef normalised(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(df.values)\n    df_new = pd.DataFrame(x_scaled, columns= df.columns)\n    return df_new\n```\n\nOr we can use the package in scikit learn of [data transformation](https://scikit-learn.org/stable/data_transforms.html). Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.\n```python\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_tranform(x_train)\nsc.tranform(x_test)\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit_tranform(x_train)\nmms.tranform(x_test)\n```\n\nUsing PCA as an example of reducing dimension\n```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\npca.fit_transdorm()\n```\n\n\n\nSpliting the data frame into train and test to varify the goodness of model\n```python\n# a fixed randome_state num will have same train and test set \ntrain_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[\"prod_id\"]], test_size=0.2, random_state=42)\n```\nLogistic Regression Classification\n```python\ndef logistic_regression_classifier(train_x, train_y):\n    model = LogisticRegression(penalty='l2', solver = \"lbfgs\", multi_class='auto')\n    model.fit(train_x, train_y)\n    return model\n\nlgr_model = logistic_regression_classifier(train_x, train_y)\npred_y = lgr_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nNaive Bayes Classification\n```python\ndef naive_bayes_classifier(x, y):\n    model = GaussianNB()\n    model.fit(x, y)\n    return model\n\nnb_model = naive_bayes_classifier(train_x, train_y)\npred_y = nb_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nKNN K-Nearest Neighbors Classification\n```python\ndef knn_classifier(x, y):\n    model = KNeighborsClassifier()\n    model.fit(train_x, train_y)\n    return model \n\nknn_model = knn_classifier(train_x, train_y)\npred_y = knn_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nRandom Forest Classifier\n```python\ndef random_forest_classifier(train_x, train_y):\n    model = RandomForestClassifier(max_depth=2, random_state=0, )\n    model.fit(train_x, train_y)\n    return model\n\nrf_model = random_forest_classifier(train_x, train_y)\nrf_model.feature_importances_  # show feature importance for each feature\nmodel = SelectFromModel(rf_model, prefit=True) # select no zero coefficient features\nx_new = model.transform(x)\nx_new.shape()\n\ntrain_x_rf = train_x.iloc[:, my_list] # can mutually define my_list = [] to select important feature\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nDecision Tree Classifier\n```python\ndef decision_tree_classifier(train_x, train_y):\n    model = tree.DecisionTreeClassifier()\n    model.fit(train_x, train_y)\n    return model\n\ndt_model = decision_tree_classifier(train_x, train_y)\npred_y = dt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nGBDT-Gradient Boosting Decision Tree Classification\n```python\ndef gradient_boosting_classifier(train_x, train_y):\n    model = GradientBoostingClassifier(n_estimators=200)\n    model.fit(train_x, train_y)\n    return model\n\ngbdt_model = gradient_boosting_classifier(train_x, train_y)\npred_y = gbdt_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```\n\nSVM-Support Vaector Machine Classification\n```python\ndef svm_classifier(train_x, train_y):\n    model = SVC(kernel='rbf', probability=True, gamma = \"auto\")\n    model.fit(train_x, train_y)\n    return model\n\nsvm_model = svm_classifier(train_x, train_y)\npred_y = svm_model.predict(test_x)\nmetrics.accuracy_score(test_y, pred_y)\n```","slug":"machine_learning","published":1,"date":"2019-07-25T03:31:57.387Z","updated":"2019-08-06T09:47:21.176Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3osost0006p1dltf1sxosb","content":"<p>Introduction of several packages in machine learning.</p>\n<a id=\"more\"></a> \n<p>Before we use many ML algorithm, we sometimes need to preprocess the data</p>\n<p>Import all package</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> statsmodels <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split <span class=\"comment\"># Import train_test_split function</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> metrics <span class=\"comment\">#Import scikit-learn metrics module for accuracy calculation</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#import model package</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier <span class=\"comment\"># Import Decision Tree Classifier</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br></pre></td></tr></table></figure>\n<h2 id=\"preprocessing\"><a class=\"markdownIt-Anchor\" href=\"#preprocessing\"></a> Preprocessing</h2>\n<p>Function to encode the data type, change all object type features into dummy variables</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encoder</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> column <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> df[column].dtype == type(object):</span><br><span class=\"line\">            le = preprocessing.LabelEncoder()</span><br><span class=\"line\">            df[column] = le.fit_transform(df[column])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br></pre></td></tr></table></figure>\n<p>Function to normalise the data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalised</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class=\"line\">    x_scaled = min_max_scaler.fit_transform(df.values)</span><br><span class=\"line\">    df_new = pd.DataFrame(x_scaled, columns= df.columns)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_new</span><br></pre></td></tr></table></figure>\n<p>Or we can use the package in scikit learn of <a href=\"https://scikit-learn.org/stable/data_transforms.html\" target=\"_blank\" rel=\"noopener\">data transformation</a>. Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit_tranform(x_train)</span><br><span class=\"line\">sc.tranform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> MinMaxScaler</span><br><span class=\"line\">mms = MinMaxScaler()</span><br><span class=\"line\">mms.fit_tranform(x_train)</span><br><span class=\"line\">mms.tranform(x_test)</span><br></pre></td></tr></table></figure>\n<p>Using PCA as an example of reducing dimension</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\">pca = PCA(n_components = <span class=\"number\">3</span>)</span><br><span class=\"line\">pca.fit_transdorm()</span><br></pre></td></tr></table></figure>\n<p>Spliting the data frame into train and test to varify the goodness of model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># a fixed randome_state num will have same train and test set </span></span><br><span class=\"line\">train_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[<span class=\"string\">\"prod_id\"</span>]], test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br></pre></td></tr></table></figure>\n<p>Logistic Regression Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">logistic_regression_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = LogisticRegression(penalty=<span class=\"string\">'l2'</span>, solver = <span class=\"string\">\"lbfgs\"</span>, multi_class=<span class=\"string\">'auto'</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">lgr_model = logistic_regression_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = lgr_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Naive Bayes Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">naive_bayes_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = GaussianNB()</span><br><span class=\"line\">    model.fit(x, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">nb_model = naive_bayes_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = nb_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>KNN K-Nearest Neighbors Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">knn_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = KNeighborsClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model </span><br><span class=\"line\"></span><br><span class=\"line\">knn_model = knn_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = knn_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Random Forest Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_forest_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = RandomForestClassifier(max_depth=<span class=\"number\">2</span>, random_state=<span class=\"number\">0</span>, )</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">rf_model = random_forest_classifier(train_x, train_y)</span><br><span class=\"line\">rf_model.feature_importances_  <span class=\"comment\"># show feature importance for each feature</span></span><br><span class=\"line\">model = SelectFromModel(rf_model, prefit=<span class=\"literal\">True</span>) <span class=\"comment\"># select no zero coefficient features</span></span><br><span class=\"line\">x_new = model.transform(x)</span><br><span class=\"line\">x_new.shape()</span><br><span class=\"line\"></span><br><span class=\"line\">train_x_rf = train_x.iloc[:, my_list] <span class=\"comment\"># can mutually define my_list = [] to select important feature</span></span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Decision Tree Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decision_tree_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = tree.DecisionTreeClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">dt_model = decision_tree_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = dt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>GBDT-Gradient Boosting Decision Tree Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_boosting_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = GradientBoostingClassifier(n_estimators=<span class=\"number\">200</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">gbdt_model = gradient_boosting_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = gbdt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>SVM-Support Vaector Machine Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">svm_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = SVC(kernel=<span class=\"string\">'rbf'</span>, probability=<span class=\"literal\">True</span>, gamma = <span class=\"string\">\"auto\"</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">svm_model = svm_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = svm_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>Introduction of several packages in machine learning.</p>","more":"<p>Before we use many ML algorithm, we sometimes need to preprocess the data</p>\n<p>Import all package</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> statsmodels <span class=\"keyword\">as</span> sm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split <span class=\"comment\"># Import train_test_split function</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> metrics <span class=\"comment\">#Import scikit-learn metrics module for accuracy calculation</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#import model package</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier <span class=\"comment\"># Import Decision Tree Classifier</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br></pre></td></tr></table></figure>\n<h2 id=\"preprocessing\"><a class=\"markdownIt-Anchor\" href=\"#preprocessing\"></a> Preprocessing</h2>\n<p>Function to encode the data type, change all object type features into dummy variables</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encoder</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> column <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> df[column].dtype == type(object):</span><br><span class=\"line\">            le = preprocessing.LabelEncoder()</span><br><span class=\"line\">            df[column] = le.fit_transform(df[column])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br></pre></td></tr></table></figure>\n<p>Function to normalise the data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalised</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class=\"line\">    x_scaled = min_max_scaler.fit_transform(df.values)</span><br><span class=\"line\">    df_new = pd.DataFrame(x_scaled, columns= df.columns)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_new</span><br></pre></td></tr></table></figure>\n<p>Or we can use the package in scikit learn of <a href=\"https://scikit-learn.org/stable/data_transforms.html\" target=\"_blank\" rel=\"noopener\">data transformation</a>. Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit_tranform(x_train)</span><br><span class=\"line\">sc.tranform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> MinMaxScaler</span><br><span class=\"line\">mms = MinMaxScaler()</span><br><span class=\"line\">mms.fit_tranform(x_train)</span><br><span class=\"line\">mms.tranform(x_test)</span><br></pre></td></tr></table></figure>\n<p>Using PCA as an example of reducing dimension</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\">pca = PCA(n_components = <span class=\"number\">3</span>)</span><br><span class=\"line\">pca.fit_transdorm()</span><br></pre></td></tr></table></figure>\n<p>Spliting the data frame into train and test to varify the goodness of model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># a fixed randome_state num will have same train and test set </span></span><br><span class=\"line\">train_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[<span class=\"string\">\"prod_id\"</span>]], test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br></pre></td></tr></table></figure>\n<p>Logistic Regression Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">logistic_regression_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = LogisticRegression(penalty=<span class=\"string\">'l2'</span>, solver = <span class=\"string\">\"lbfgs\"</span>, multi_class=<span class=\"string\">'auto'</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">lgr_model = logistic_regression_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = lgr_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Naive Bayes Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">naive_bayes_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = GaussianNB()</span><br><span class=\"line\">    model.fit(x, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">nb_model = naive_bayes_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = nb_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>KNN K-Nearest Neighbors Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">knn_classifier</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    model = KNeighborsClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model </span><br><span class=\"line\"></span><br><span class=\"line\">knn_model = knn_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = knn_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Random Forest Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_forest_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = RandomForestClassifier(max_depth=<span class=\"number\">2</span>, random_state=<span class=\"number\">0</span>, )</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">rf_model = random_forest_classifier(train_x, train_y)</span><br><span class=\"line\">rf_model.feature_importances_  <span class=\"comment\"># show feature importance for each feature</span></span><br><span class=\"line\">model = SelectFromModel(rf_model, prefit=<span class=\"literal\">True</span>) <span class=\"comment\"># select no zero coefficient features</span></span><br><span class=\"line\">x_new = model.transform(x)</span><br><span class=\"line\">x_new.shape()</span><br><span class=\"line\"></span><br><span class=\"line\">train_x_rf = train_x.iloc[:, my_list] <span class=\"comment\"># can mutually define my_list = [] to select important feature</span></span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>Decision Tree Classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decision_tree_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = tree.DecisionTreeClassifier()</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">dt_model = decision_tree_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = dt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>GBDT-Gradient Boosting Decision Tree Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_boosting_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = GradientBoostingClassifier(n_estimators=<span class=\"number\">200</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">gbdt_model = gradient_boosting_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = gbdt_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>\n<p>SVM-Support Vaector Machine Classification</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">svm_classifier</span><span class=\"params\">(train_x, train_y)</span>:</span></span><br><span class=\"line\">    model = SVC(kernel=<span class=\"string\">'rbf'</span>, probability=<span class=\"literal\">True</span>, gamma = <span class=\"string\">\"auto\"</span>)</span><br><span class=\"line\">    model.fit(train_x, train_y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br><span class=\"line\">svm_model = svm_classifier(train_x, train_y)</span><br><span class=\"line\">pred_y = svm_model.predict(test_x)</span><br><span class=\"line\">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>"},{"title":"Read data","_content":"Include a method to read(scratch) data from website and transfer them into dataframe. \n<!--more-->\n\n### csv file\nUsing Pandas package  [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\", encoding = \"utf-8\") # check encoding type such like \"utf-16\"\n ```\n\n### data from website\nReading data about house price and house feature from a website as an example:\n\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef soup_to_df(s):\n    #define column name\n    dfcols = ['outcode', 'last_published_date','latitude', 'longitude', 'post_town', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n    df_xml = pd.DataFrame(columns=dfcols)\n\n    for node in s.find_all(\"listing\"):\n        outcode =  node.find('outcode').get_text()\n        last_published_date = node.find('last_published_date').get_text()\n        latitude = node.find('latitude').get_text()\n        longitude = node.find('longitude').get_text()\n        post_town = node.find('post_town').get_text()\n        num_bathrooms = node.find('num_bathrooms').get_text()\n        num_bedrooms = node.find('num_bedrooms').get_text()\n        num_floors = node.find('num_floors').get_text()\n        num_recepts = node.find('num_recepts').get_text()\n        property_type = node.find('property_type').get_text()\n        street_name = node.find('street_name').get_text()\n        price = node.find('price').get_text()\n\n        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),\n                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=True)   \n    return df_xml\n\ndfcols = ['outcode', 'latitude', 'longitude', 'post_town', 'last_published_date', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n\ndf_all = pd.DataFrame(columns=dfcols)\n\nfor i in range(1, 58): #page range\n# Ctrl+Shift+I in that website to find the url for which we would like to scrape. \n    baseurl = f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&county=Somerset&country=England&listing_status=sale&include_sold=1&page_number={i}&page_size=100\"\n    page = requests.get(baseurl)\n    soup = BeautifulSoup(page.content)\n    #save data from one page into a dataframe\n    df = soup_to_df(soup)\n    # combine all data from all pages into one dataframe\n    df_all = pd.concat([df_all, df], axis = 0) \n```\n","source":"_posts/read_data.md","raw":"---\ntitle: Read data\n---\nInclude a method to read(scratch) data from website and transfer them into dataframe. \n<!--more-->\n\n### csv file\nUsing Pandas package  [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\", encoding = \"utf-8\") # check encoding type such like \"utf-16\"\n ```\n\n### data from website\nReading data about house price and house feature from a website as an example:\n\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef soup_to_df(s):\n    #define column name\n    dfcols = ['outcode', 'last_published_date','latitude', 'longitude', 'post_town', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n    df_xml = pd.DataFrame(columns=dfcols)\n\n    for node in s.find_all(\"listing\"):\n        outcode =  node.find('outcode').get_text()\n        last_published_date = node.find('last_published_date').get_text()\n        latitude = node.find('latitude').get_text()\n        longitude = node.find('longitude').get_text()\n        post_town = node.find('post_town').get_text()\n        num_bathrooms = node.find('num_bathrooms').get_text()\n        num_bedrooms = node.find('num_bedrooms').get_text()\n        num_floors = node.find('num_floors').get_text()\n        num_recepts = node.find('num_recepts').get_text()\n        property_type = node.find('property_type').get_text()\n        street_name = node.find('street_name').get_text()\n        price = node.find('price').get_text()\n\n        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),\n                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=True)   \n    return df_xml\n\ndfcols = ['outcode', 'latitude', 'longitude', 'post_town', 'last_published_date', 'num_bathrooms', 'num_bedrooms', 'num_floors', \n              'num_recepts', 'property_type', 'street_name', \"price\"]\n\ndf_all = pd.DataFrame(columns=dfcols)\n\nfor i in range(1, 58): #page range\n# Ctrl+Shift+I in that website to find the url for which we would like to scrape. \n    baseurl = f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&county=Somerset&country=England&listing_status=sale&include_sold=1&page_number={i}&page_size=100\"\n    page = requests.get(baseurl)\n    soup = BeautifulSoup(page.content)\n    #save data from one page into a dataframe\n    df = soup_to_df(soup)\n    # combine all data from all pages into one dataframe\n    df_all = pd.concat([df_all, df], axis = 0) \n```\n","slug":"read_data","published":1,"date":"2019-07-25T03:31:57.387Z","updated":"2019-08-06T09:44:43.575Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3ososu0007p1dlf91easyv","content":"<p>Include a method to read(scratch) data from website and transfer them into dataframe.</p>\n<a id=\"more\"></a>\n<h3 id=\"csv-file\"><a class=\"markdownIt-Anchor\" href=\"#csv-file\"></a> csv file</h3>\n<p>Using Pandas package  <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\" target=\"_blank\" rel=\"noopener\">read_csv</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">\"data.csv\"</span>, encoding = <span class=\"string\">\"utf-8\"</span>) <span class=\"comment\"># check encoding type such like \"utf-16\"</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"data-from-website\"><a class=\"markdownIt-Anchor\" href=\"#data-from-website\"></a> data from website</h3>\n<p>Reading data about house price and house feature from a website as an example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soup_to_df</span><span class=\"params\">(s)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\">#define column name</span></span><br><span class=\"line\">    dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'last_published_date'</span>,<span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\">    df_xml = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> node <span class=\"keyword\">in</span> s.find_all(<span class=\"string\">\"listing\"</span>):</span><br><span class=\"line\">        outcode =  node.find(<span class=\"string\">'outcode'</span>).get_text()</span><br><span class=\"line\">        last_published_date = node.find(<span class=\"string\">'last_published_date'</span>).get_text()</span><br><span class=\"line\">        latitude = node.find(<span class=\"string\">'latitude'</span>).get_text()</span><br><span class=\"line\">        longitude = node.find(<span class=\"string\">'longitude'</span>).get_text()</span><br><span class=\"line\">        post_town = node.find(<span class=\"string\">'post_town'</span>).get_text()</span><br><span class=\"line\">        num_bathrooms = node.find(<span class=\"string\">'num_bathrooms'</span>).get_text()</span><br><span class=\"line\">        num_bedrooms = node.find(<span class=\"string\">'num_bedrooms'</span>).get_text()</span><br><span class=\"line\">        num_floors = node.find(<span class=\"string\">'num_floors'</span>).get_text()</span><br><span class=\"line\">        num_recepts = node.find(<span class=\"string\">'num_recepts'</span>).get_text()</span><br><span class=\"line\">        property_type = node.find(<span class=\"string\">'property_type'</span>).get_text()</span><br><span class=\"line\">        street_name = node.find(<span class=\"string\">'street_name'</span>).get_text()</span><br><span class=\"line\">        price = node.find(<span class=\"string\">'price'</span>).get_text()</span><br><span class=\"line\"></span><br><span class=\"line\">        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),</span><br><span class=\"line\">                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=<span class=\"literal\">True</span>)   </span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_xml</span><br><span class=\"line\"></span><br><span class=\"line\">dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'last_published_date'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">df_all = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">58</span>): <span class=\"comment\">#page range</span></span><br><span class=\"line\"><span class=\"comment\"># Ctrl+Shift+I in that website to find the url for which we would like to scrape. </span></span><br><span class=\"line\">    baseurl = <span class=\"string\">f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&amp;county=Somerset&amp;country=England&amp;listing_status=sale&amp;include_sold=1&amp;page_number=<span class=\"subst\">&#123;i&#125;</span>&amp;page_size=100\"</span></span><br><span class=\"line\">    page = requests.get(baseurl)</span><br><span class=\"line\">    soup = BeautifulSoup(page.content)</span><br><span class=\"line\">    <span class=\"comment\">#save data from one page into a dataframe</span></span><br><span class=\"line\">    df = soup_to_df(soup)</span><br><span class=\"line\">    <span class=\"comment\"># combine all data from all pages into one dataframe</span></span><br><span class=\"line\">    df_all = pd.concat([df_all, df], axis = <span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>Include a method to read(scratch) data from website and transfer them into dataframe.</p>","more":"<h3 id=\"csv-file\"><a class=\"markdownIt-Anchor\" href=\"#csv-file\"></a> csv file</h3>\n<p>Using Pandas package  <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\" target=\"_blank\" rel=\"noopener\">read_csv</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">\"data.csv\"</span>, encoding = <span class=\"string\">\"utf-8\"</span>) <span class=\"comment\"># check encoding type such like \"utf-16\"</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"data-from-website\"><a class=\"markdownIt-Anchor\" href=\"#data-from-website\"></a> data from website</h3>\n<p>Reading data about house price and house feature from a website as an example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soup_to_df</span><span class=\"params\">(s)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\">#define column name</span></span><br><span class=\"line\">    dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'last_published_date'</span>,<span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\">    df_xml = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> node <span class=\"keyword\">in</span> s.find_all(<span class=\"string\">\"listing\"</span>):</span><br><span class=\"line\">        outcode =  node.find(<span class=\"string\">'outcode'</span>).get_text()</span><br><span class=\"line\">        last_published_date = node.find(<span class=\"string\">'last_published_date'</span>).get_text()</span><br><span class=\"line\">        latitude = node.find(<span class=\"string\">'latitude'</span>).get_text()</span><br><span class=\"line\">        longitude = node.find(<span class=\"string\">'longitude'</span>).get_text()</span><br><span class=\"line\">        post_town = node.find(<span class=\"string\">'post_town'</span>).get_text()</span><br><span class=\"line\">        num_bathrooms = node.find(<span class=\"string\">'num_bathrooms'</span>).get_text()</span><br><span class=\"line\">        num_bedrooms = node.find(<span class=\"string\">'num_bedrooms'</span>).get_text()</span><br><span class=\"line\">        num_floors = node.find(<span class=\"string\">'num_floors'</span>).get_text()</span><br><span class=\"line\">        num_recepts = node.find(<span class=\"string\">'num_recepts'</span>).get_text()</span><br><span class=\"line\">        property_type = node.find(<span class=\"string\">'property_type'</span>).get_text()</span><br><span class=\"line\">        street_name = node.find(<span class=\"string\">'street_name'</span>).get_text()</span><br><span class=\"line\">        price = node.find(<span class=\"string\">'price'</span>).get_text()</span><br><span class=\"line\"></span><br><span class=\"line\">        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),</span><br><span class=\"line\">                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=<span class=\"literal\">True</span>)   </span><br><span class=\"line\">    <span class=\"keyword\">return</span> df_xml</span><br><span class=\"line\"></span><br><span class=\"line\">dfcols = [<span class=\"string\">'outcode'</span>, <span class=\"string\">'latitude'</span>, <span class=\"string\">'longitude'</span>, <span class=\"string\">'post_town'</span>, <span class=\"string\">'last_published_date'</span>, <span class=\"string\">'num_bathrooms'</span>, <span class=\"string\">'num_bedrooms'</span>, <span class=\"string\">'num_floors'</span>, </span><br><span class=\"line\">              <span class=\"string\">'num_recepts'</span>, <span class=\"string\">'property_type'</span>, <span class=\"string\">'street_name'</span>, <span class=\"string\">\"price\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">df_all = pd.DataFrame(columns=dfcols)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">58</span>): <span class=\"comment\">#page range</span></span><br><span class=\"line\"><span class=\"comment\"># Ctrl+Shift+I in that website to find the url for which we would like to scrape. </span></span><br><span class=\"line\">    baseurl = <span class=\"string\">f\"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&amp;county=Somerset&amp;country=England&amp;listing_status=sale&amp;include_sold=1&amp;page_number=<span class=\"subst\">&#123;i&#125;</span>&amp;page_size=100\"</span></span><br><span class=\"line\">    page = requests.get(baseurl)</span><br><span class=\"line\">    soup = BeautifulSoup(page.content)</span><br><span class=\"line\">    <span class=\"comment\">#save data from one page into a dataframe</span></span><br><span class=\"line\">    df = soup_to_df(soup)</span><br><span class=\"line\">    <span class=\"comment\"># combine all data from all pages into one dataframe</span></span><br><span class=\"line\">    df_all = pd.concat([df_all, df], axis = <span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>"},{"title":"Timeseries","date":"2019-07-25T06:51:27.000Z","_content":"---\nIntroduction of ARIMA and SARIMA model \n<!--more-->\n\n## ARIMA-autoregreesive integrated moving average\n\nBefore we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p > \\alpha, we say the process is not stationary at \\alpha significant level. \n\nIn the common time series model, we have autoregressive (AR) model--AR(p), moving average (MA) model--MA(q), autoregressive–moving-average (ARMA)--ARMA(p,q) and Autoregressive integrated moving average (ARIMA)--ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.  \n\n- AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)\n- I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1\n- MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.\n\nThe way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter. \n\nIn the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.\n\nTo find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.\n \n## Seasonal autoregressive integrated moving average model(SARIMA) \nSARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.\n\nThe general formula for SARIMA with seasonal period as 12(months per year) is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\"/>\n</p> \n\nwhere \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\"/>\n</p> \n\nSince we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.\n\nTo compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.\n\nSome comparison:\n\n| ARIMA model | mean square error |\n| ----------- | -------------- |\n| ARIMA(3,0,1)(2,1,2,12) |  275 |\n| ARIMA(3,1,2)(2,1,2,12) |  279 |\n| ARIMA(3,0,2)(0,1,2,12) |  260 |\n| ARIMA(3,0,2)(1,1,2,12) |  266 |\n| ARIMA(3,0,1)(1,1,2,12) |  276 |\n| ARIMA(2,0,1)(0,1,2,12) |  268 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(3,1,1)(0,1,2,12) |  270 |\n...\n\nThen ARIMA(3,0,2)(0,1,2,12) may be better here.\n","source":"_posts/timeseries.md","raw":"---\ntitle: Timeseries\ndate: 2019-07-25 14:51:27\ntags:\n---\n---\nIntroduction of ARIMA and SARIMA model \n<!--more-->\n\n## ARIMA-autoregreesive integrated moving average\n\nBefore we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p > \\alpha, we say the process is not stationary at \\alpha significant level. \n\nIn the common time series model, we have autoregressive (AR) model--AR(p), moving average (MA) model--MA(q), autoregressive–moving-average (ARMA)--ARMA(p,q) and Autoregressive integrated moving average (ARIMA)--ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.  \n\n- AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)\n- I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1\n- MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.\n\nThe way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter. \n\nIn the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.\n\nTo find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.\n \n## Seasonal autoregressive integrated moving average model(SARIMA) \nSARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.\n\nThe general formula for SARIMA with seasonal period as 12(months per year) is:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\"/>\n</p> \n\nwhere \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\"/>\n</p> \n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\"/>\n</p> \n\nSince we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.\n\nTo compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.\n\nSome comparison:\n\n| ARIMA model | mean square error |\n| ----------- | -------------- |\n| ARIMA(3,0,1)(2,1,2,12) |  275 |\n| ARIMA(3,1,2)(2,1,2,12) |  279 |\n| ARIMA(3,0,2)(0,1,2,12) |  260 |\n| ARIMA(3,0,2)(1,1,2,12) |  266 |\n| ARIMA(3,0,1)(1,1,2,12) |  276 |\n| ARIMA(2,0,1)(0,1,2,12) |  268 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(2,1,1)(0,1,2,12) |  271 |\n| ARIMA(3,1,1)(0,1,2,12) |  270 |\n...\n\nThen ARIMA(3,0,2)(0,1,2,12) may be better here.\n","slug":"timeseries","published":1,"updated":"2019-08-06T09:45:26.672Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3ososv0008p1dlmq0q37mg","content":"<hr>\n<p>Introduction of ARIMA and SARIMA model</p>\n<a id=\"more\"></a>\n<h2 id=\"arima-autoregreesive-integrated-moving-average\"><a class=\"markdownIt-Anchor\" href=\"#arima-autoregreesive-integrated-moving-average\"></a> ARIMA-autoregreesive integrated moving average</h2>\n<p>Before we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p &gt; \\alpha, we say the process is not stationary at \\alpha significant level.</p>\n<p>In the common time series model, we have autoregressive (AR) model–AR§, moving average (MA) model–MA(q), autoregressive–moving-average (ARMA)–ARMA(p,q) and Autoregressive integrated moving average (ARIMA)–ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.</p>\n<ul>\n<li>AR: Auto-Regressive §: AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)</li>\n<li>I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1</li>\n<li>MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.</li>\n</ul>\n<p>The way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter.</p>\n<p>In the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.</p>\n<p>To find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.</p>\n<h2 id=\"seasonal-autoregressive-integrated-moving-average-modelsarima\"><a class=\"markdownIt-Anchor\" href=\"#seasonal-autoregressive-integrated-moving-average-modelsarima\"></a> Seasonal autoregressive integrated moving average model(SARIMA)</h2>\n<p>SARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.</p>\n<p>The general formula for SARIMA with seasonal period as 12(months per year) is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\">\n</p> \n<p>where</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\">\n</p> \n<p>Since we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.</p>\n<p>To compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.</p>\n<p>Some comparison:</p>\n<table>\n<thead>\n<tr>\n<th>ARIMA model</th>\n<th>mean square error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ARIMA(3,0,1)(2,1,2,12)</td>\n<td>275</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,2)(2,1,2,12)</td>\n<td>279</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(0,1,2,12)</td>\n<td>260</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(1,1,2,12)</td>\n<td>266</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,1)(1,1,2,12)</td>\n<td>276</td>\n</tr>\n<tr>\n<td>ARIMA(2,0,1)(0,1,2,12)</td>\n<td>268</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,1)(0,1,2,12)</td>\n<td>270</td>\n</tr>\n</tbody>\n</table>\n<p>…</p>\n<p>Then ARIMA(3,0,2)(0,1,2,12) may be better here.</p>\n","site":{"data":{}},"excerpt":"<hr>\n<p>Introduction of ARIMA and SARIMA model</p>","more":"<h2 id=\"arima-autoregreesive-integrated-moving-average\"><a class=\"markdownIt-Anchor\" href=\"#arima-autoregreesive-integrated-moving-average\"></a> ARIMA-autoregreesive integrated moving average</h2>\n<p>Before we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p &gt; \\alpha, we say the process is not stationary at \\alpha significant level.</p>\n<p>In the common time series model, we have autoregressive (AR) model–AR§, moving average (MA) model–MA(q), autoregressive–moving-average (ARMA)–ARMA(p,q) and Autoregressive integrated moving average (ARIMA)–ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.</p>\n<ul>\n<li>AR: Auto-Regressive §: AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)</li>\n<li>I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1</li>\n<li>MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.</li>\n</ul>\n<p>The way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter.</p>\n<p>In the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.</p>\n<p>To find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.</p>\n<h2 id=\"seasonal-autoregressive-integrated-moving-average-modelsarima\"><a class=\"markdownIt-Anchor\" href=\"#seasonal-autoregressive-integrated-moving-average-modelsarima\"></a> Seasonal autoregressive integrated moving average model(SARIMA)</h2>\n<p>SARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.</p>\n<p>The general formula for SARIMA with seasonal period as 12(months per year) is:</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\phi(B)\\Phi(B^{12})y_t = \\theta_0+\\theta(B)\\Theta(B^{12})\\epsilon_t\">\n</p> \n<p>where</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; AR: \\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space; MA: \\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal AR: \\Phi(B^S) = 1 - \\Phi_1B^S - ... - \\Phi_PB^PS\">\n</p> \n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Seasonal MA: \\Theta(B^S) = 1 + \\Theta_1B^S + ... + \\Theta_QB^QS\">\n</p> \n<p>Since we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.</p>\n<p>To compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.</p>\n<p>Some comparison:</p>\n<table>\n<thead>\n<tr>\n<th>ARIMA model</th>\n<th>mean square error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ARIMA(3,0,1)(2,1,2,12)</td>\n<td>275</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,2)(2,1,2,12)</td>\n<td>279</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(0,1,2,12)</td>\n<td>260</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,2)(1,1,2,12)</td>\n<td>266</td>\n</tr>\n<tr>\n<td>ARIMA(3,0,1)(1,1,2,12)</td>\n<td>276</td>\n</tr>\n<tr>\n<td>ARIMA(2,0,1)(0,1,2,12)</td>\n<td>268</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(2,1,1)(0,1,2,12)</td>\n<td>271</td>\n</tr>\n<tr>\n<td>ARIMA(3,1,1)(0,1,2,12)</td>\n<td>270</td>\n</tr>\n</tbody>\n</table>\n<p>…</p>\n<p>Then ARIMA(3,0,2)(0,1,2,12) may be better here.</p>"}],"PostAsset":[{"_id":"source/_posts/linear-model/orth.png","post":"cjz3osost0005p1dl2v3bgoeg","slug":"orth.png","modified":1,"renderable":1}],"PostCategory":[],"PostTag":[],"Tag":[]}}